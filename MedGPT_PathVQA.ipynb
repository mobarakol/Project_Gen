{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/Project_Gen/blob/main/MedGPT_PathVQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvn6yKZCMHhU"
      },
      "source": [
        "Downlaod code and dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install peft accelerate datasets\n",
        "!pip -q install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip -q install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "zOp7_2C1u0o6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705f7f4e-32eb-47a6-cd6e-bda6985356f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U evaluate bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlypc7GYvFar",
        "outputId": "8719b521-cf97-434c-b9b3-f7ab10384d90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading PathVQA dataset:(manually)<br>\n",
        "Downloading PathVQA dataset:(HF)<br>\n",
        "```\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "datasets.config.DOWNLOADED_DATASETS_PATH = \"vqa_datasets\"\n",
        "dataset = load_dataset(\"flaviagiammarino/path-vqa\")\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']\n",
        "```"
      ],
      "metadata": {
        "id": "4cQvhKpJ6ZEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir vqa_datasets\n",
        "!gdown --id 1eDts_DGDrSRIGl7ajQZOip30cBP4J55x\n",
        "!mv pvqa.zip vqa_datasets\n",
        "!unzip -q vqa_datasets/pvqa.zip -d vqa_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3R25Oq738I2",
        "outputId": "5770f9cc-08dc-47ec-847f-2d56dff24688"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1eDts_DGDrSRIGl7ajQZOip30cBP4J55x\n",
            "From (redirected): https://drive.google.com/uc?id=1eDts_DGDrSRIGl7ajQZOip30cBP4J55x&confirm=t&uuid=8b755d3c-1670-4819-933c-e4d428ccac72\n",
            "To: /content/pvqa.zip\n",
            "100% 1.80G/1.80G [00:18<00:00, 99.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the PathVQA for GPT training:"
      ],
      "metadata": {
        "id": "FWugxS-gUXb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from transformers import GPT2Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "def update_classes(pkl_train, pkl_val, pkl_test):\n",
        "    # standardize answer ids across datasets and compute the maximum number of generated output tokens based on the train set\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    with open(pkl_train, 'rb') as f:\n",
        "            data_train = pickle.load(f)\n",
        "    with open(pkl_val, 'rb') as f:\n",
        "            data_val = pickle.load(f)\n",
        "    with open(pkl_test, 'rb') as f:\n",
        "            data_test = pickle.load(f)\n",
        "\n",
        "    cur_id = 0\n",
        "    class_names_list = []\n",
        "    class_ids_list = [[],[],[]]\n",
        "\n",
        "    for i, data in enumerate([data_train,data_val,data_test]):\n",
        "\n",
        "        for answer in data['answers']:\n",
        "            if answer not in class_names_list:\n",
        "                class_names_list.append(answer)\n",
        "                class_ids_list[i].append(cur_id)\n",
        "                cur_id+=1\n",
        "            else:\n",
        "                class_ids_list[i].append(class_names_list.index(answer))\n",
        "    q_lens = []\n",
        "    a_lens = []\n",
        "    for question in data_train['questions']:\n",
        "        q_lens.append(len(tokenizer.encode(question)))\n",
        "    for answer in data_train['answers']:\n",
        "        a_lens.append(len(tokenizer.encode(str(answer))))\n",
        "\n",
        "    data_train['class_ids'] = class_ids_list[0]\n",
        "    data_val['class_ids'] = class_ids_list[1]\n",
        "    data_test['class_ids'] = class_ids_list[2]\n",
        "\n",
        "    data_train['class_names'] = class_names_list\n",
        "    data_val['class_names'] = class_names_list\n",
        "    data_test['class_names'] = class_names_list\n",
        "\n",
        "    data_train['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "    data_val['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "    data_test['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "\n",
        "    with open(pkl_train, 'wb') as f:\n",
        "        pickle.dump(data_train,f)\n",
        "    with open(pkl_val, 'wb') as f:\n",
        "        pickle.dump(data_val,f)\n",
        "    with open(pkl_test, 'wb') as f:\n",
        "        pickle.dump(data_test,f)\n",
        "\n",
        "def preprocess_pathvqa(split, out_path):\n",
        "    device = torch.device('cuda:0')\n",
        "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "    data =  pd.read_pickle('vqa_datasets/pvqa/qas/{}/{}_qa.pkl'.format(split,split))\n",
        "    print(\"%0d captions loaded from json \" % len(data))\n",
        "    all_img_prefixes = []\n",
        "    img_ids = []\n",
        "    img_paths = []\n",
        "    all_questions = []\n",
        "    all_answers = []\n",
        "    img_dict = {}\n",
        "    for i in tqdm(range(len(data))):\n",
        "        d = data[i]\n",
        "        if d['answer']!=\"yes\" and d['answer']!=\"no\":\n",
        "            img_id = d[\"image\"]\n",
        "            filename = \"vqa_datasets/pvqa/images/{}/{}.jpg\".format(split,img_id)\n",
        "            with torch.no_grad():\n",
        "                prefix_i = clip_model.encode_image(preprocess(Image.open(filename)).unsqueeze(0).to(device)).cpu()\n",
        "            if img_id not in img_dict.keys():\n",
        "                img_dict[img_id] = [[d['question']],[d['answer']],prefix_i,filename]\n",
        "            else:\n",
        "                img_dict[img_id][0].append(d['question'])\n",
        "                img_dict[img_id][1].append(d['answer'])\n",
        "    # this dictionary is converted into a format that is sutiable for the data loader. Each data point contains a 'img_id', that corresponds is the index of the corresponding\n",
        "    # CLIP embedding of the image in 'img_prefix'.\n",
        "    for img_id, imgs in enumerate(img_dict.keys()):\n",
        "        all_img_prefixes.append(img_dict[imgs][2])\n",
        "        for q in range(len(img_dict[imgs][0])):\n",
        "            all_questions.append(img_dict[imgs][0][q])\n",
        "            all_answers.append(img_dict[imgs][1][q])\n",
        "            img_ids.append(img_id)\n",
        "            img_paths.append(img_dict[imgs][3])\n",
        "\n",
        "    all_data = {\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers,'img_path': img_paths}\n",
        "\n",
        "    with open(out_path, 'wb') as f:\n",
        "        pickle.dump(all_data,f)\n",
        "    print('Done')\n",
        "    print(\"%0d embeddings saved \" % len(all_img_prefixes))\n",
        "\n",
        "\n",
        "for split in ['train','val','test']:\n",
        "    out_path = \"vqa_datasets/pvqa/{}.pkl\".format(split)\n",
        "    preprocess_pathvqa(split,out_path)\n",
        "\n",
        "update_classes(\"vqa_datasets/pvqa/train.pkl\", \"vqa_datasets/pvqa/val.pkl\", \"vqa_datasets/pvqa/test.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562,
          "referenced_widgets": [
            "c70a7b6bb5e24de5806992d944c27291",
            "ada8ddea08fb4478af6ed5dee2d7701e",
            "4638476a140640fd9dad6b5194af7c4d",
            "a4763d323d264c9888fe714b0cb45988",
            "18928a34e77f4040bad531e315f225ae",
            "da18c4555d9b4fb2ad55e987e281ae5d",
            "929210dd40e64cf489f4e7c8ecab67da",
            "89870a2cec2c46b196229241f21ecddd",
            "c04613a8b1894d5592b5bba5dab9d0e3",
            "4beec6a669f8490cb2b41f9fb114b9db",
            "2432adb15cfc4a32a15e76145410a6f9",
            "3222092179a841a394b8843da12df282",
            "aa2161d6e4e04e808c819933f8bfc82e",
            "016e356e1e424a089ef3f46407830be5",
            "fa97c34b296f4f2ea1ea7440df05f739",
            "dcdb0db163024a65a14e3c28fc665851",
            "04b376eb0c4e40b4921ce865e0174c49",
            "de8f1ade3185424a8005940d7e075c79",
            "5b19086950b1412b9221bbdcb35eabdc",
            "1041e5cfa34540df9f2edbadf1ccff0e",
            "08e508029ab148c7847a74149b7d50b0",
            "76cae1cbbd67463c91021fb8a659ada9",
            "eb5fe23ceec64e7ba941ca7bfca7dc16",
            "bc374dde6aee44e3bb5ebcbe0de8bf9d",
            "f5d69252771748fcb40b49e6e70f965c",
            "86e5482006db4905bed7f5e83c1a81de",
            "492b9556522f48f1bf12c60c909ec996",
            "215389ddee41441fbf647bb6a2e82ab6",
            "cc50b0cadd1148e3ab0ebd4048cde499",
            "e86d1f64aa3245daa695962fbbed574e",
            "09efd816efd64d1d9966c91e28743920",
            "ef6645e93a6247cfa75d67e5998c24f6",
            "d1163551c041471b87f42d2b11c98dff",
            "aa65472b5af54476907df8cb779f8395",
            "460d8b8b8fd04f9590cf6ae13b94f055",
            "dadaaffebb2b40dfb5ee0888ebc60b7c",
            "bc188e832b584e6189ac809669b96f17",
            "507bba83cbc34446abad7a920c1c5e86",
            "e575b0c2a3ec4facb937eb436edf7bca",
            "0e23f012d1e94f35b4b51e243b8a384c",
            "40bc61745905499dbba6a03dddffa125",
            "1751277d1e2847c1941b2a5406b71bc6",
            "0da89bcece0542bebb304e345f41f1af",
            "ee38c5aa3aaa49fbbfbd8b3fd2d4ce64",
            "f721be874179430d8972c535b685d3bd",
            "6ac009af6f3348a6afe2eaaa0d265275",
            "680a2bec05e34f21b0f6d33807ef382f",
            "5b94c044f02549e387ff8f0be4594c2b",
            "eb9d08180c4a4240b8b1b5b806194892",
            "8106c2126da64a2aa25a8b72731c4909",
            "8235a09671b340b1bec9a349ceb4a9f9",
            "4c730263b7e9454192ac8f1a870f5b03",
            "c2f77d153d21490f8e56d1cb5908a040",
            "06fea263db744e84a12119f6a0335d9f",
            "e25305db5ea54e5ebdba117ae4e67cbb"
          ]
        },
        "id": "I-E1SL5YDkgJ",
        "outputId": "16bf014c-c439-48b9-b984-3ae5330bfb38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:02<00:00, 136MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19755 captions loaded from json \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19755/19755 [05:04<00:00, 64.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n",
            "2577 embeddings saved \n",
            "6279 captions loaded from json \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6279/6279 [01:34<00:00, 66.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n",
            "821 embeddings saved \n",
            "6761 captions loaded from json \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6761/6761 [01:56<00:00, 58.02it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n",
            "849 embeddings saved \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c70a7b6bb5e24de5806992d944c27291"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3222092179a841a394b8843da12df282"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb5fe23ceec64e7ba941ca7bfca7dc16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa65472b5af54476907df8cb779f8395"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f721be874179430d8972c535b685d3bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training Process"
      ],
      "metadata": {
        "id": "qa84IGuYjm-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed, GPT2Config, GPT2Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.biogpt import BioGptTokenizer\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import pdb\n",
        "import argparse\n",
        "\n",
        "class medvqaDataset(Dataset):\n",
        "    def __init__(self, path, split='train',like_test=False,prefix_length=2,model_type = 'gpt2'):\n",
        "        super().__init__()\n",
        "        data_path = path+split+'.pkl'\n",
        "        with open(data_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        sys.stdout.flush()\n",
        "        self.model_type = model_type\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.img_ids = data[\"img_ids\"]\n",
        "        self.img_prefixes = data[\"img_prefix\"]\n",
        "        self.questions = data['questions']\n",
        "        self.answers = data['answers']\n",
        "        self.img_paths = data['img_path']\n",
        "\n",
        "        self.max_seqs_len = data['max_seqs_len']\n",
        "        self.labels = data['class_ids']\n",
        "        self.train_setting = True if (split!='test'and like_test==False) else False\n",
        "        self.prefix_len = prefix_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.answers)\n",
        "    def pad_sequences(self,index):\n",
        "        m = [torch.tensor(self.tokenizer.encode('question: ')),torch.tensor(self.tokenizer.encode(' context:')),torch.tensor(self.tokenizer.encode('answer ')),torch.tensor(self.tokenizer.encode('<|endoftext|>'))]\n",
        "        m_mask = [torch.ones(len(self.tokenizer.encode('question: '))),torch.ones(len(self.tokenizer.encode(' context:'))),torch.ones(len(self.tokenizer.encode('answer '))),torch.zeros(len(self.tokenizer.encode('<|endoftext|>')))]\n",
        "\n",
        "        if self.train_setting:\n",
        "            # construct the model input. The order is question, image, answer. During training the answer is masked. Any padding is placed on the right of the sequence.\n",
        "            # placeholder tokens are used on the location where the visual prefix will be inserted, with q_len indicating this location.\n",
        "            q=torch.tensor(self.tokenizer.encode(self.questions[index]))\n",
        "            a=torch.tensor(self.tokenizer.encode(str(self.answers[index])))\n",
        "\n",
        "            q,q_mask,leftover_tokens = self.make_padding(self.max_seqs_len[0],q,question=True)\n",
        "            q_len = m[0].size(0) + q.size(0) + m[1].size(0)\n",
        "            a,a_mask,_ = self.make_padding(self.max_seqs_len[1],a,leftover_tokens=leftover_tokens)\n",
        "            if len((a==0).nonzero())!=0:\n",
        "                pad_start = (a==0).nonzero()[0]\n",
        "            else:\n",
        "                pad_start=[]\n",
        "            a = torch.cat((a,m[3])) if len(pad_start)==0 else torch.cat((a[:pad_start],m[3],a[pad_start:]))\n",
        "            q = torch.cat((m[0],q,m[1],torch.ones(self.prefix_len),m[2],a))\n",
        "\n",
        "            q_mask = torch.cat((m_mask[0],q_mask,m_mask[1],torch.ones(self.prefix_len),m_mask[2],a_mask,m_mask[3]))\n",
        "            return q,q_mask, q_len\n",
        "        else:\n",
        "            # in the test stage we do not have acces to the answer, so we just load the question.\n",
        "            # since inference is not performed batch-wised we don't need to apply padding\n",
        "            q = torch.tensor(self.tokenizer.encode(self.questions[index]))\n",
        "\n",
        "            q,q_mask,_ = self.make_padding_test_setting(self.max_seqs_len[0],q)\n",
        "            q_len = m[0].size(0) + q.size(0) + m[1].size(0)\n",
        "            q = torch.cat((m[0],q,m[1],torch.ones(self.prefix_len),m[2]))\n",
        "\n",
        "\n",
        "            q_mask = torch.cat((m_mask[0],q_mask,m_mask[1]))\n",
        "            return q,q_mask,q_len\n",
        "\n",
        "    def make_padding(self, max_len, tokens, question=False,leftover_tokens=0):\n",
        "        padding = max_len - tokens.size(0)\n",
        "        if padding > 0:\n",
        "            if question:\n",
        "                leftover_tokens = padding\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "            else:\n",
        "                tokens = torch.cat((tokens, torch.zeros(padding+leftover_tokens)))\n",
        "                mask = torch.zeros(max_len+leftover_tokens)\n",
        "\n",
        "        elif padding==0:\n",
        "            if question:\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "            else:\n",
        "                mask = torch.zeros(tokens.size(0)+leftover_tokens)\n",
        "                tokens = torch.cat((tokens,torch.zeros(leftover_tokens)))\n",
        "\n",
        "\n",
        "        elif padding < 0:\n",
        "            if question:\n",
        "                tokens = tokens[:max_len]\n",
        "                mask = torch.ones(max_len)\n",
        "            else:\n",
        "                tokens = torch.cat((tokens[:max_len], torch.zeros(leftover_tokens)))\n",
        "                mask = torch.zeros(max_len+ leftover_tokens)\n",
        "        return tokens, mask, leftover_tokens\n",
        "    def make_padding_test_setting(self, max_len, tokens,do_padding=False):\n",
        "        padding = max_len - tokens.size(0)\n",
        "        padding_len = 0\n",
        "        if padding > 0:\n",
        "            if do_padding:\n",
        "                mask = torch.cat((torch.ones(tokens.size(0)),torch.zeros(padding)))\n",
        "                tokens = torch.cat((tokens,torch.zeros(padding)))\n",
        "                padding_len = padding\n",
        "            else:\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "        elif padding ==0:\n",
        "            mask = torch.ones(max_len)\n",
        "        elif padding < 0:\n",
        "            tokens = tokens[:max_len]\n",
        "            mask = torch.ones(max_len)\n",
        "        return tokens, mask, padding_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        prefix = self.img_prefixes[self.img_ids[index]]\n",
        "        tokens, mask, q_len  = self.pad_sequences(index)\n",
        "        return prefix,  self.labels[index], tokens, mask, q_len\n",
        "\n"
      ],
      "metadata": {
        "id": "41Bf8MRc3F28"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart the session, please:"
      ],
      "metadata": {
        "id": "HvqeL8w6wp0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "import sys\n",
        "import transformers\n",
        "from transformers import set_seed, GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers.models.biogpt import BioGptForCausalLM, BioGptTokenizer, BioGptConfig\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\n",
        "from peft import LoraConfig, get_peft_model,get_peft_config,PeftModelForCausalLM,TaskType,PrefixTuningConfig, PromptEncoderConfig, PromptTuningConfig\n",
        "import random\n",
        "\n",
        "#eval\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "# from utils import generate_beam\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import GPT2Tokenizer\n",
        "import pdb\n",
        "from evaluate import load\n",
        "import collections\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(nn.Dropout(p=0.5))\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "class VQAmedModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        prefix_length=2,\n",
        "        clip_length=2,\n",
        "        prefix_size=512,\n",
        "        num_layers=8,\n",
        "        setting=\"lora\",\n",
        "        mapping_type=\"MLP\",\n",
        "        args=None,\n",
        "    ):\n",
        "        super(VQAmedModel, self).__init__()\n",
        "        gpttype = args.model_type\n",
        "        self.gpttype = gpttype\n",
        "        self.model_type = gpttype\n",
        "        self.setting = setting\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = AutoModelForCausalLM.from_pretrained(gpttype,load_in_8bit=True,device_map='auto')\n",
        "        # load the relevant fine-tuning strategy\n",
        "        if setting == \"lora\":\n",
        "            peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"prefixtuning\":\n",
        "            peft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"p_tuning\":\n",
        "            peft_config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"prompttuning\":\n",
        "            peft_config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting=='frozen':\n",
        "            for param in self.gpt.transformer.parameters():\n",
        "                param.requires_grad = False\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpttype)\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if mapping_type == \"MLP\":\n",
        "            self.clip_project = MLP((\n",
        "                    prefix_size,\n",
        "                    (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                    self.gpt_embedding_size * prefix_length,\n",
        "                    self.gpt_embedding_size * prefix_length))\n",
        "        # elif mapping_type == \"Transformer\":\n",
        "        #     self.clip_project = TransformerMapper(\n",
        "        #         prefix_size,\n",
        "        #         self.gpt_embedding_size,\n",
        "        #         prefix_length,\n",
        "        #         clip_length,\n",
        "        #         num_layers)\n",
        "        else:\n",
        "            raise ValueError(\"select valid mapping type: MLP or Transformer\")\n",
        "\n",
        "    def forward(self, prefix, labels, tokens, mask, q_len, batch_size):\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        if self.gpttype=='microsoft/biogpt':\n",
        "            embedding = self.gpt.transformer.embed_tokens(tokens)\n",
        "        else:\n",
        "            embedding = self.gpt.transformer.wte(tokens)\n",
        "        for b in range(batch_size):\n",
        "            # insert the visual prefix after the question\n",
        "            embedding[b,q_len[b]:q_len[b]+self.prefix_length,:] = prefix_projections[b]\n",
        "        return self.gpt(inputs_embeds=embedding, attention_mask=mask)\n",
        "    def generate(self, prefix, labels, tokens, mask, q_len):\n",
        "        prefix_projections = self.clip_project(prefix.view(1, -1)).view(self.prefix_length, self.gpt_embedding_size)\n",
        "        if self.gpttype=='microsoft/biogpt':\n",
        "            embedding_txt = self.gpt.transformer.embed_tokens(tokens)\n",
        "        else:\n",
        "            embedding_txt = self.gpt.transformer.wte(tokens)\n",
        "        embedding_txt[q_len:q_len+self.prefix_length,:] = prefix_projections\n",
        "        return embedding_txt\n",
        "\n",
        "\n",
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "def treebank_tokenize(s):\n",
        "    return TreebankWordTokenizer().tokenize(s)\n",
        "def generate_beam(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    beam_size: int = 5,\n",
        "    generated=None,\n",
        "    entry_length=65,\n",
        "    temperature=1.0,\n",
        "    stop_token: str = \"<|endoftext|>\",\n",
        "):\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "            logits = logits.softmax(-1).log()\n",
        "            # final_logit\n",
        "\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(\n",
        "                    beam_size, -1\n",
        "                )\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            if model.model_type == \"biogpt\":\n",
        "                next_token_embed = model.gpt.biogpt.embed_tokens(\n",
        "                    next_tokens.squeeze()\n",
        "                ).view(generated.shape[0], 1, -1)\n",
        "            elif model.model_type == \"gpt2\":\n",
        "                next_token_embed = model.gpt.transformer.wte(\n",
        "                    next_tokens.squeeze()\n",
        "                ).view(generated.shape[0], 1, -1)\n",
        "            else:\n",
        "                next_token_embed = model.gpt.get_input_embeddings()(tokens[:,-1])\n",
        "                next_token_embed=next_token_embed.squeeze().view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [\n",
        "        tokenizer.decode(output[: int(length)])\n",
        "        for output, length in zip(output_list, seq_lengths)\n",
        "    ]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "# from utils import generate_beam\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import GPT2Tokenizer\n",
        "import pdb\n",
        "from evaluate import load\n",
        "import collections\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "\n",
        "def eval_gpt_open_ended(model, dataset, args, print_vis_token_meaning=False):\n",
        "    model.eval()\n",
        "    model=model.cuda()\n",
        "    bert_score = load(\"bertscore\")\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model.model_type)\n",
        "    bleu_avg1=0.\n",
        "    bert_avg1 = 0.\n",
        "    bert_avg2 = 0.\n",
        "    bert_avg3 = 0.\n",
        "    f1_avg = 0.\n",
        "    acc = 0.\n",
        "    acc_oe = 0.\n",
        "    acc_yn = 0.\n",
        "    c_oe =1e-9\n",
        "    c_yn =1e-9\n",
        "    with tqdm(total=len(dataset)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(\"Testing\")\n",
        "        for item in range(len(dataset)):\n",
        "            prefix,  labels, tokens, mask, q_len = dataset[item]\n",
        "            prefix = prefix.type(torch.float32).cuda()\n",
        "            tokens = tokens.type(torch.long).cuda()\n",
        "            mask = mask.cuda()\n",
        "            with autocast(dtype=torch.float16):\n",
        "              with torch.no_grad():\n",
        "                  embed = model.generate(prefix,labels,tokens,mask,q_len).view(1,tokens.size(0),-1)\n",
        "                  if print_vis_token_meaning:\n",
        "                    prefix_projections = embed[:,q_len:q_len+model.prefix_length,:]\n",
        "                    for i in range(prefix_projections.size(1)):\n",
        "                      print_nearest_text_token(prefix_projections[0,i], model)\n",
        "                  out_text = generate_beam(model, model.tokenizer,generated=embed,entry_length=dataset.max_seqs_len[1], temperature=1)[0]\n",
        "\n",
        "            if out_text.lower()==dataset.answers[item].lower():\n",
        "              acc+=1\n",
        "            if dataset.answers[item].lower()=='yes' or dataset.answers[item].lower()=='no':\n",
        "              if out_text.lower()==dataset.answers[item].lower():\n",
        "                acc_yn+=1\n",
        "              c_yn+=1\n",
        "            else:\n",
        "              if out_text.lower()==dataset.answers[item].lower():\n",
        "                acc_oe+=1\n",
        "              c_oe+=1\n",
        "\n",
        "            reference = [str(dataset.answers[item])]\n",
        "            candidate = [out_text]\n",
        "\n",
        "            bleu_1 = sentence_bleu(reference[0], candidate[0], weights=(1, 0, 0, 0))\n",
        "\n",
        "            a = bert_score.compute(references = reference,predictions = candidate,model_type = 'bert-base-uncased')\n",
        "            bert_avg1+= a['precision'][0]\n",
        "            bert_avg2+= a['recall'][0]\n",
        "            bert_avg3+= a['f1'][0]\n",
        "\n",
        "\n",
        "            f1_avg += compute_f1(tokenizer.encode(reference[0]),tokenizer.encode(candidate[0]))\n",
        "            bleu_avg1+=bleu_1\n",
        "\n",
        "\n",
        "    print('------------')\n",
        "    print(\"BLEU {}\".format(round(bleu_avg1/len(dataset),3)))\n",
        "    print(\"BERTScore {}\".format(round(bert_avg3/len(dataset),3)))\n",
        "    print(\"F1 {}\".format(round(f1_avg/len(dataset),3)))\n",
        "    print(\"Accuracy {}\".format(round(acc/len(dataset),3)))\n",
        "    print(\"Accuracy YN{}\".format(round(acc_yn/c_yn,3)))\n",
        "    print(\"Accuracy OE{}\".format(round(acc_oe/c_oe,3)))\n",
        "\n",
        "def print_nearest_text_token(vis_token, model):\n",
        "    \"\"\"print the nearest token in the vocabulary to the given token through model.gpt.embeddings.weight\"\"\"\n",
        "    embeddings = model.gpt.transformer.wte.weight\n",
        "    distances = torch.norm(embeddings - vis_token, dim=1)\n",
        "    nearest_token_idx = torch.argmin(distances)\n",
        "    print(model.tokenizer.decode([nearest_token_idx.item()]))\n",
        "\n",
        "def compute_f1(gold_toks, pred_toks):\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "\n",
        "def pytorch_model_run(train_loader, valid_loader, test_dataset, model_obj, args):\n",
        "    accelerator = Accelerator()\n",
        "    device = accelerator.device\n",
        "\n",
        "    if not os.path.exists(args.out_dir):\n",
        "        os.makedirs(args.out_dir)\n",
        "\n",
        "    model = model_obj.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=args.warmup_steps,\n",
        "        num_training_steps=args.epochs * len(train_loader),\n",
        "    )\n",
        "\n",
        "    ## ğŸ’¡ introduce all components to accelerate library\n",
        "    model, optimizer, train_loader, scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_loader, scheduler\n",
        "    )\n",
        "    valid_loader = accelerator.prepare(valid_loader)\n",
        "\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    counter = 0\n",
        "    n_epochs = args.epochs\n",
        "    accelerator.wait_for_everyone()\n",
        "    for epoch in range(args.epochs):\n",
        "        model.to(device)\n",
        "        with tqdm(total=args.batch_size * len(train_loader)) as epoch_pbar:\n",
        "            epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
        "            start_time = time.time()\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "\n",
        "            for i, (prefix, labels, tokens, mask, q_len) in enumerate(train_loader):\n",
        "                with accelerator.accumulate(model):\n",
        "                    prefix = prefix.type(torch.float32)\n",
        "                    tokens = tokens.type(torch.long)\n",
        "                    mask = mask.type(torch.long)\n",
        "                    q_len = q_len.type(torch.long)\n",
        "                    outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                    logits = outputs.logits\n",
        "                    loss = 0.\n",
        "\n",
        "                    shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "\n",
        "                    for b in range(logits.size(0)):\n",
        "                        condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                        condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "\n",
        "                        loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                    loss=loss/logits.size(0)\n",
        "\n",
        "                    accelerator.backward(loss)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    total_loss += loss.item()\n",
        "                    avg_loss = total_loss / (i+1)\n",
        "                    desc = f\"Epoch {epoch} - loss {avg_loss:.20f}\"\n",
        "                    epoch_pbar.set_description(desc)\n",
        "                    epoch_pbar.update(prefix.shape[0])\n",
        "        model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        with tqdm(total=args.batch_size * len(valid_loader)) as epoch_pbar:\n",
        "            epoch_pbar.set_description(f\"VAL Epoch {epoch}\")\n",
        "            for i, (prefix, labels, tokens, mask,q_len) in enumerate(valid_loader):\n",
        "                torch.cuda.empty_cache()\n",
        "                prefix = prefix.type(torch.float32)\n",
        "                tokens = tokens.type(torch.long)\n",
        "                mask = mask.type(torch.long)\n",
        "                q_len = q_len.type(torch.long)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                    logits = outputs.logits\n",
        "                    loss = 0.\n",
        "                    shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "                    for b in range(logits.size(0)):\n",
        "                        condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                        condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "                        loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                    loss=loss/logits.size(0)\n",
        "                    total_loss += loss.item()\n",
        "                avg_val_loss = total_loss / (i + 1)\n",
        "                desc = f\"VAL Epoch {epoch} - loss {avg_val_loss:.20f}\"\n",
        "                epoch_pbar.set_description(desc)\n",
        "                epoch_pbar.update(prefix.shape[0])\n",
        "\n",
        "        if avg_val_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_val_loss\n",
        "\n",
        "            torch.save(model.state_dict(), os.path.join(args.out_dir, f\"open_ended_latest.pt\"))\n",
        "\n",
        "        scheduler.step()\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(\n",
        "            \"VAL epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s\".format(\n",
        "                epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time\n",
        "            )\n",
        "        )\n",
        "        if avg_val_loss > avg_loss:\n",
        "            counter += 1\n",
        "        if counter == 5:\n",
        "            break\n",
        "\n",
        "        eval_gpt_open_ended(model, test_dataset, args, print_vis_token_meaning=False)\n",
        "    return model"
      ],
      "metadata": {
        "id": "RrdshczXrKUo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.nn.functional as nnf\n",
        "from accelerate import Accelerator\n",
        "import os\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "def parse_argument():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "    parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "    parser.add_argument(\"--ablation\", type=str, default=\"none\", choices=(\"remove_question\", \"remove_visual\",'replace_visual',\"swap\"))\n",
        "    parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "    parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "    parser.add_argument(\"--dataset_path\", type=str, default=\"vqa_datasets/\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=20)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "    parser.add_argument(\"--dataset\", type=str, default='pathvqa', choices=('pathvqa', 'ovqa', 'slake'))\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "    parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--iters_to_accumulate\", type=int, default=4)\n",
        "    parser.add_argument(\"--validation_step\", type=int, default=1000)\n",
        "    parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "    parser.add_argument(\"--checkpoint\", type=str)\n",
        "    parser.add_argument(\"--eval\", dest=\"eval\", action=\"store_true\")\n",
        "    parser.add_argument(\"--verbose\", dest=\"verbose\", action=\"store_true\")\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    set_random_seeds(args.seed)\n",
        "    return args\n",
        "\n",
        "args = parse_argument()\n",
        "\n",
        "suffix = f\"v5_prefixlength_{args.prefix_length}_mapping_{args.mapping_type}_seed_{args.seed}_gpttype_{args.model_type.replace('/','')}_setting_{args.setting}_dataset_{args.dataset}\"\n",
        "\n",
        "args.out_dir = os.path.join('checkpoints', suffix)\n",
        "\n",
        "train_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"train\",prefix_length=args.prefix_length,model_type=args.model_type)#,abl=args.ablation)\n",
        "val_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"val\",prefix_length=args.prefix_length,model_type=args.model_type)#, abl=args.ablation)\n",
        "test_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"test\",prefix_length=args.prefix_length,model_type=args.model_type,like_test=True)\n",
        "print('Sample: Tain={}, valid={}, Test={}'.format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "model = VQAmedModel(\n",
        "            prefix_length=args.prefix_length,\n",
        "            clip_length=4,\n",
        "            setting=args.setting,\n",
        "            mapping_type=args.mapping_type,\n",
        "            args=args,\n",
        "        )\n",
        "\n",
        "model = pytorch_model_run(train_dataloader, val_dataloader, test_dataset, model, args)\n"
      ],
      "metadata": {
        "id": "EZMBItk-oR2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27de9fb2-c1c3-4da3-bb28-2f3c835e1d0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample: Tain=9949, valid=3144, Test=3370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = VQAmedModel(\n",
        "#             prefix_length=args.prefix_length,\n",
        "#             clip_length=4,\n",
        "#             setting=args.setting,\n",
        "#             mapping_type=args.mapping_type,\n",
        "#             args=args,\n",
        "#         )\n",
        "\n",
        "checkpoint = os.path.join(args.out_dir, f\"open_ended_latest.pt\")\n",
        "if args.verbose:\n",
        "    print(f\">> Loading pre-trained model {checkpoint}!\")\n",
        "if os.path.exists(checkpoint):\n",
        "    print('ckpt exist')\n",
        "    model.cuda()\n",
        "    model.load_state_dict(\n",
        "        torch.load(checkpoint, map_location=torch.device(\"cuda\")), strict=False\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(\"Please provide valid path for loading checkpoint\")\n",
        "eval_gpt_open_ended(model, test_dataset, args, print_vis_token_meaning=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSa2eVmOzcNy",
        "outputId": "ec4498b9-cb6f-4e4a-b3db-4fd0b27fd54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ckpt exist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Testing:   0%|          | 0/3370 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "parser.add_argument('--model_ver',      default= 'efvlegpt2rs18', )\n",
        "parser.add_argument(\"--dataset\", type=str, default='pathvqa', choices=('pathvqa', 'ovqa', 'slake'))\n",
        "parser.add_argument(\"--dataset_path\", type=str, default=\"vqa_datasets/\")\n",
        "args = parser.parse_args([])\n",
        "\n",
        "\n",
        "model = VQAmedModel(\n",
        "            prefix_length=args.prefix_length,\n",
        "            clip_length=4,\n",
        "            setting=args.setting,\n",
        "            mapping_type=args.mapping_type,\n",
        "            args=args,\n",
        "        )\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3btm9efh-ip",
        "outputId": "28ffd897-30dd-43ba-9c14-ceaf65364152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.nn.functional as nnf\n",
        "from accelerate import Accelerator\n",
        "import os\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "parser.add_argument('--model_ver',      default= 'efvlegpt2rs18', )\n",
        "parser.add_argument(\"--dataset\", type=str, default='pathvqa', choices=('pathvqa', 'ovqa', 'slake'))\n",
        "parser.add_argument(\"--dataset_path\", type=str, default=\"vqa_datasets/\")\n",
        "args = parser.parse_args([])\n",
        "\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "if not os.path.exists(args.out_dir):\n",
        "    os.makedirs(args.out_dir)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=args.warmup_steps,\n",
        "    num_training_steps=args.epochs * len(train_loader),\n",
        ")\n",
        "\n",
        "valid_loader = accelerator.prepare(valid_loader)\n",
        "\n",
        "best_valid_loss = float(\"inf\")\n",
        "counter = 0\n",
        "n_epochs = args.epochs\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    with tqdm(total=args.batch_size * len(train_loader)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0.0\n",
        "        total_rocauc = 0.0\n",
        "        for i, (prefix, labels, tokens, mask, q_len) in enumerate(train_loader):\n",
        "            with accelerator.accumulate(model):\n",
        "                prefix = prefix.type(torch.float32).to(device)\n",
        "                tokens = tokens.type(torch.long).to(device)\n",
        "                mask = mask.type(torch.long).to(device)\n",
        "                q_len = q_len.type(torch.long).to(device)\n",
        "                outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                logits = outputs.logits\n",
        "                loss = 0.\n",
        "\n",
        "                shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "\n",
        "                for b in range(logits.size(0)):\n",
        "                    condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                    condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "\n",
        "                    loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                loss=loss/logits.size(0)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                total_loss += loss.item()\n",
        "                avg_loss = total_loss / (i+1)\n",
        "                avg_acc = total_acc / (i + 1)\n",
        "                avg_roc = total_rocauc / (i + 1)\n",
        "                desc = f\"Epoch {epoch} - loss {avg_loss:.20f} -accuracy {avg_acc:.4f} -auc {avg_roc:.4f}\"\n",
        "                epoch_pbar.set_description(desc)\n",
        "                epoch_pbar.update(prefix.shape[0])\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    with tqdm(total=args.batch_size * len(valid_loader)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(f\"VAL Epoch {epoch}\")\n",
        "        for i, (prefix, labels, tokens, mask,q_len) in enumerate(valid_loader):\n",
        "            torch.cuda.empty_cache()\n",
        "            prefix = prefix.type(torch.float32).to(device)\n",
        "            tokens = tokens.type(torch.long).to(device)\n",
        "            mask = mask.type(torch.long).to(device)\n",
        "            q_len = q_len.type(torch.long).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                logits = outputs.logits\n",
        "                loss = 0.\n",
        "                shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "                for b in range(logits.size(0)):\n",
        "                    condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                    condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "                    loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                loss=loss/logits.size(0)\n",
        "                total_loss += loss.item()\n",
        "            avg_val_loss = total_loss / (i + 1)\n",
        "            avg_acc = total_acc / (i + 1)\n",
        "            avg_roc = total_rocauc / (i + 1)\n",
        "            desc = f\"VAL Epoch {epoch} - loss {avg_val_loss:.20f} -acc {avg_acc:.4f} -roc {avg_roc:.4f}\"\n",
        "            epoch_pbar.set_description(desc)\n",
        "            epoch_pbar.update(prefix.shape[0])\n",
        "\n",
        "    if avg_val_loss < best_valid_loss:\n",
        "        best_valid_loss = avg_val_loss\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(args.out_dir, f\"open_ended_latest.pt\"))\n",
        "\n",
        "    scheduler.step()\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\n",
        "        \"VAL epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s\".format(\n",
        "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time\n",
        "        )\n",
        "    )\n",
        "    if avg_val_loss > avg_loss:\n",
        "        counter += 1\n",
        "    if counter == 5:\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "id": "fV3_o0ScxDI6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "203ce600-c1b7-47fd-e53d-749941f4bc17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 0 - loss 4.39535212862318847016 -accuracy 0.0000 -auc 0.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9936/9936 [08:49<00:00, 18.78it/s]\n",
            "VAL Epoch 0 - loss 3.00385642538265296153 -acc 0.0000 -roc 0.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3136/3136 [01:08<00:00, 45.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL epoch 1/30 \t loss=4.3954 \t val_loss=3.0039 \t time=613.67s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - loss 2.98002032844387754196 -accuracy 0.0000 -auc 0.0000:  16%|â–ˆâ–Œ        | 1568/9936 [01:21<07:14, 19.27it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-323fd224ee52>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2125\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data  import DataLoader\n",
        "\n",
        "class EndoVis18VQAGPTSentence(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head = '../dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa2/Sentence/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.Resize((300,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                q_s, an_s = line.split('|')\n",
        "                q_s = q_s.split('&')\n",
        "                an_s = an_s.split(('&'))\n",
        "                for i in range(len(q_s)):\n",
        "                    q_a = q_s[i]+'|'+an_s[i]\n",
        "                    # print(file, q_a)\n",
        "                    self.vqas.append([file, q_a])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img loc[3],\n",
        "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        answer = '<|sep|> '+answer\n",
        "\n",
        "\n",
        "        return img, question, answer\n",
        "\n",
        "\n",
        "# data location\n",
        "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "val_seq = [1, 5, 16]\n",
        "\n",
        "folder_head = 'datasets_my/EndoVis-18-VQA/seq_'\n",
        "folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "valid_loader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "print('training sample:', len(train_dataset), 'val sample:', len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8GMeoGEy5Hr",
        "outputId": "ca656db6-52c8-4cee-f7c4-6bf8b8f59d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "training sample: 10574 val sample: 3216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2RoeUOhPErX"
      },
      "source": [
        "Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvA3l8GpPG7y"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLSnki7sPIfM"
      },
      "source": [
        "Running Script:<br>\n",
        "%cd /content/Project_Gen\n",
        "import os\n",
        "os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "!python train_SGPT_V2_Sentence.py \\\n",
        "--lr=0.00001 \\\n",
        "--checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_' \\\n",
        "--dataset_type='m18' \\\n",
        "--tokenizer_ver='gpt2v1' \\\n",
        "--model_ver='efvlegpt2rs18' \\\n",
        "--model_subver='v1' \\\n",
        "--vis_pos_emb='zeroes'\\\n",
        "--batch_size=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48GwkJQvPArS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ac10d4-0cb5-446d-fdb9-24b3bd8cbfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project_Gen\n",
            "device = cuda\n",
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "model params:  174607680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/265 [03:20<03:18,  1.57s/it]"
          ]
        }
      ],
      "source": [
        "%cd /content/Project_Gen\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch.backends.cudnn as cudnn\n",
        "# from model.EFGPT2Sentence import EFVLEGPT2RS18Sentence\n",
        "# from dataloader.dataloaderGPT2Sentence import EndoVis18VQAGPTSentence\n",
        "# from utils import AverageMeter, save_clf_checkpoint, adjust_learning_rate\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from transformers import  VisualBertConfig, GPT2Config\n",
        "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
        "from transformers import GPT2LMHeadModel\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import ViTFeatureExtractor, AutoFeatureExtractor\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def save_clf_checkpoint(checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, Acc):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'Acc': Acc,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer}\n",
        "    filename = checkpoint_dir + 'Best.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "\n",
        "class EndoVis18VQAGPTSentence(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head = '../dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa2/Sentence/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.Resize((300,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                q_s, an_s = line.split('|')\n",
        "                q_s = q_s.split('&')\n",
        "                an_s = an_s.split(('&'))\n",
        "                for i in range(len(q_s)):\n",
        "                    q_a = q_s[i]+'|'+an_s[i]\n",
        "                    # print(file, q_a)\n",
        "                    self.vqas.append([file, q_a])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img loc[3],\n",
        "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        answer = '<|sep|> '+answer\n",
        "\n",
        "\n",
        "        return img, question, answer\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class EFVLEGPT2RS18Sentence(nn.Module):\n",
        "    def __init__(self, model_subver = 'v3', tokenizer_len=50258, vis_pos_emb = None):\n",
        "        super(EFVLEGPT2RS18Sentence, self).__init__()\n",
        "        '''\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
        "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
        "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
        "        '''\n",
        "\n",
        "        self.sub_ver = model_subver\n",
        "        self.vis_pos_emb = vis_pos_emb\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
        "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "        self.img_feature_extractor.fc = new_fc\n",
        "        self.visual_embedder = nn.Linear(512, 768)\n",
        "\n",
        "        ## word_embedding default GPT2 word embedding\n",
        "        gpt2configuration = GPT2Config()\n",
        "        word_embedder = GPT2Model(gpt2configuration)\n",
        "        word_embedder.resize_token_embeddings(tokenizer_len)\n",
        "        self.word_embedder = word_embedder.wte\n",
        "\n",
        "        ## GPT2 visual context aware decoder\n",
        "        self.VCAdecoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "    def forward(self, question, img, answer):\n",
        "\n",
        "        ## image encoder features\n",
        "        img_feature = self.img_feature_extractor(img)\n",
        "        img_feature = torch.unsqueeze(img_feature, dim=1)\n",
        "\n",
        "        ## visual Embedding : id type 1, pos: zero / incremental\n",
        "        visual_embeds = self.visual_embedder(img_feature)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "        visual_attention_mask = visual_attention_mask.to(device)\n",
        "\n",
        "        ## question embedding:\n",
        "        question['input_ids'] = question['input_ids'].to(device)\n",
        "        question_embeds = self.word_embedder(question['input_ids'])\n",
        "        question_attention_mask = question['attention_mask'].to(device)\n",
        "\n",
        "        ## answer embedding\n",
        "        answer['input_ids'] = answer['input_ids'].to(device)\n",
        "        answer_embeds = self.word_embedder(answer['input_ids'])\n",
        "        answer_attention_mask = answer['attention_mask'].to(device)\n",
        "\n",
        "        ## token type of zero and position id for question\n",
        "        question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        question_position_id = torch.arange(0,question_embeds.size()[1])\n",
        "        question_position_id = torch.unsqueeze(question_position_id,0)\n",
        "        question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
        "        question_position_id = question_position_id.to(device)\n",
        "        question_len = len(question_position_id[0])\n",
        "        visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        visual_len = len(visual_position_id[0])\n",
        "        answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
        "        answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
        "        answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
        "        answer_position_id += (question_len+visual_len)\n",
        "        answer_position_id = answer_position_id.to(device)\n",
        "\n",
        "        ## question first\n",
        "        inputs_embeds = torch.cat((question_embeds, visual_embeds, answer_embeds), dim=1)\n",
        "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask, answer_attention_mask), dim=1)\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            token_type_ids = torch.cat((question_id_type, visual_id_type, answer_id_type), dim=1)\n",
        "            position_ids = torch.cat((question_position_id, visual_position_id, answer_position_id), dim=1)\n",
        "\n",
        "\n",
        "        ## VCA_GPT2 decoder\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
        "        else:\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, ( visual_features, questions, answers) in enumerate(tqdm(train_dataloader),0):\n",
        "\n",
        "        # prepare questions and answers\n",
        "        question_list = []\n",
        "        answer_list = []\n",
        "        for question in questions: question_list.append(question)\n",
        "        for answer in answers: answer_list.append(answer)\n",
        "\n",
        "        question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "        answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "        # Visual features\n",
        "        visual_features = visual_features.to(device)\n",
        "        visual_len = 80\n",
        "\n",
        "        # model forward(question, img, answer)\n",
        "        logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "        # only consider loss on reference summary just like seq2seq models\n",
        "        idx = args.question_len + 1\n",
        "\n",
        "        shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "        shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "        shift_labels = shift_labels.to(device)\n",
        "\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "\n",
        "    print(\"Epoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (visual_features, questions, answers) in enumerate(tqdm(val_loader),0):\n",
        "\n",
        "            # prepare questions and answers\n",
        "            question_list = []\n",
        "            answer_list = []\n",
        "            for question in questions: question_list.append(question)\n",
        "            for answer in answers: answer_list.append(answer)\n",
        "\n",
        "            if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "                answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = visual_features.to(device)\n",
        "                visual_len = 80\n",
        "\n",
        "            # model forward(question, img, answer)\n",
        "            logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "\n",
        "            # only consider loss on reference summary just like seq2seq models\n",
        "            idx = args.question_len + 1\n",
        "            shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "            shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "\n",
        "            # copy for logits and labels for sentence decoding and blue-4 score calculation\n",
        "            logits_copy = logits.clone()\n",
        "            shift_labels_copy = shift_labels.clone()\n",
        "\n",
        "            # loss calculation\n",
        "            shift_labels = shift_labels.to(device)\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # references    - Ground truth answer\n",
        "            answer_GT_dec = tokenizer.batch_decode(shift_labels_copy, skip_special_tokens= True)\n",
        "            for answer_GT_dec_i in answer_GT_dec: references.append([answer_GT_dec_i.split()])\n",
        "            # print(references)\n",
        "\n",
        "            # Hypotheses - predicted answer\n",
        "            _, answer_Gen_id = torch.max(logits_copy, dim=2)\n",
        "            answer_Gen_dec = tokenizer.batch_decode(answer_Gen_id, skip_special_tokens= True)\n",
        "            for answer_Gen_dec_i in answer_Gen_dec: hypotheses.append(answer_Gen_dec_i.split())\n",
        "            # print(hypotheses)\n",
        "\n",
        "\n",
        "        # Calculate BLEU1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "        print(\"Epoch: {}/{} EVA LOSS: {:.6f} BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
        "          (epoch, args.epochs, total_loss.avg, metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=50,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                            help=' 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2rs18/m18/v3_p_qf_',      help='m18/c80')\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                         help='m18/c80')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--model_subver',   default= 'v3',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--answer_len',     default= 35,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2rs18',                               help='efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--vis_pos_emb',    default= 'pos',                                         help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    args = get_arg()\n",
        "    args.lr = 0.00005\n",
        "    args.epochs = 2\n",
        "    args.checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_'\n",
        "    args.dataset_type='m18'\n",
        "    args.tokenizer_ver='gpt2v1'\n",
        "    args.model_ver='efvlegpt2rs18'\n",
        "    args.model_subver='v1'\n",
        "    args.vis_pos_emb='zeroes'\n",
        "    args.batch_size=40\n",
        "    os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer_length = len(tokenizer)\n",
        "\n",
        "    # data location\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = 'datasets/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "    val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "    model = EFVLEGPT2RS18Sentence(model_subver = args.model_subver, tokenizer_len=len(tokenizer), vis_pos_emb = args.vis_pos_emb)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    #evsluation\n",
        "    # checkpoint = torch.load(args.checkpoint, map_location=str(device))\n",
        "    # start_epoch = checkpoint['epoch']\n",
        "    # epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "    # model = checkpoint['model']\n",
        "    # optimizer = checkpoint['optimizer']\n",
        "    # final_args = checkpoint['final_args']\n",
        "    # for key in final_args.keys(): args.__setattr__(key, final_args[key])\n",
        "\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    model = model.to(device)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = CrossEntropyLoss().to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        # validation\n",
        "        metrics = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "            best_results[0] = metrics[\"Bleu_4\"]\n",
        "            best_epoch[0] = epoch\n",
        "            save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0])\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sph3wTEeJueY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c70a7b6bb5e24de5806992d944c27291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ada8ddea08fb4478af6ed5dee2d7701e",
              "IPY_MODEL_4638476a140640fd9dad6b5194af7c4d",
              "IPY_MODEL_a4763d323d264c9888fe714b0cb45988"
            ],
            "layout": "IPY_MODEL_18928a34e77f4040bad531e315f225ae"
          }
        },
        "ada8ddea08fb4478af6ed5dee2d7701e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da18c4555d9b4fb2ad55e987e281ae5d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_929210dd40e64cf489f4e7c8ecab67da",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "4638476a140640fd9dad6b5194af7c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89870a2cec2c46b196229241f21ecddd",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c04613a8b1894d5592b5bba5dab9d0e3",
            "value": 26
          }
        },
        "a4763d323d264c9888fe714b0cb45988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4beec6a669f8490cb2b41f9fb114b9db",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2432adb15cfc4a32a15e76145410a6f9",
            "value": "â€‡26.0/26.0â€‡[00:00&lt;00:00,â€‡1.90kB/s]"
          }
        },
        "18928a34e77f4040bad531e315f225ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da18c4555d9b4fb2ad55e987e281ae5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "929210dd40e64cf489f4e7c8ecab67da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89870a2cec2c46b196229241f21ecddd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c04613a8b1894d5592b5bba5dab9d0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4beec6a669f8490cb2b41f9fb114b9db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2432adb15cfc4a32a15e76145410a6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3222092179a841a394b8843da12df282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa2161d6e4e04e808c819933f8bfc82e",
              "IPY_MODEL_016e356e1e424a089ef3f46407830be5",
              "IPY_MODEL_fa97c34b296f4f2ea1ea7440df05f739"
            ],
            "layout": "IPY_MODEL_dcdb0db163024a65a14e3c28fc665851"
          }
        },
        "aa2161d6e4e04e808c819933f8bfc82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04b376eb0c4e40b4921ce865e0174c49",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_de8f1ade3185424a8005940d7e075c79",
            "value": "vocab.json:â€‡100%"
          }
        },
        "016e356e1e424a089ef3f46407830be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b19086950b1412b9221bbdcb35eabdc",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1041e5cfa34540df9f2edbadf1ccff0e",
            "value": 1042301
          }
        },
        "fa97c34b296f4f2ea1ea7440df05f739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08e508029ab148c7847a74149b7d50b0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_76cae1cbbd67463c91021fb8a659ada9",
            "value": "â€‡1.04M/1.04Mâ€‡[00:00&lt;00:00,â€‡3.91MB/s]"
          }
        },
        "dcdb0db163024a65a14e3c28fc665851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b376eb0c4e40b4921ce865e0174c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de8f1ade3185424a8005940d7e075c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b19086950b1412b9221bbdcb35eabdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1041e5cfa34540df9f2edbadf1ccff0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08e508029ab148c7847a74149b7d50b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76cae1cbbd67463c91021fb8a659ada9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb5fe23ceec64e7ba941ca7bfca7dc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc374dde6aee44e3bb5ebcbe0de8bf9d",
              "IPY_MODEL_f5d69252771748fcb40b49e6e70f965c",
              "IPY_MODEL_86e5482006db4905bed7f5e83c1a81de"
            ],
            "layout": "IPY_MODEL_492b9556522f48f1bf12c60c909ec996"
          }
        },
        "bc374dde6aee44e3bb5ebcbe0de8bf9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_215389ddee41441fbf647bb6a2e82ab6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cc50b0cadd1148e3ab0ebd4048cde499",
            "value": "merges.txt:â€‡100%"
          }
        },
        "f5d69252771748fcb40b49e6e70f965c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e86d1f64aa3245daa695962fbbed574e",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09efd816efd64d1d9966c91e28743920",
            "value": 456318
          }
        },
        "86e5482006db4905bed7f5e83c1a81de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef6645e93a6247cfa75d67e5998c24f6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d1163551c041471b87f42d2b11c98dff",
            "value": "â€‡456k/456kâ€‡[00:00&lt;00:00,â€‡2.79MB/s]"
          }
        },
        "492b9556522f48f1bf12c60c909ec996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215389ddee41441fbf647bb6a2e82ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc50b0cadd1148e3ab0ebd4048cde499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e86d1f64aa3245daa695962fbbed574e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09efd816efd64d1d9966c91e28743920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef6645e93a6247cfa75d67e5998c24f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1163551c041471b87f42d2b11c98dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa65472b5af54476907df8cb779f8395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_460d8b8b8fd04f9590cf6ae13b94f055",
              "IPY_MODEL_dadaaffebb2b40dfb5ee0888ebc60b7c",
              "IPY_MODEL_bc188e832b584e6189ac809669b96f17"
            ],
            "layout": "IPY_MODEL_507bba83cbc34446abad7a920c1c5e86"
          }
        },
        "460d8b8b8fd04f9590cf6ae13b94f055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e575b0c2a3ec4facb937eb436edf7bca",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0e23f012d1e94f35b4b51e243b8a384c",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "dadaaffebb2b40dfb5ee0888ebc60b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40bc61745905499dbba6a03dddffa125",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1751277d1e2847c1941b2a5406b71bc6",
            "value": 1355256
          }
        },
        "bc188e832b584e6189ac809669b96f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0da89bcece0542bebb304e345f41f1af",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ee38c5aa3aaa49fbbfbd8b3fd2d4ce64",
            "value": "â€‡1.36M/1.36Mâ€‡[00:00&lt;00:00,â€‡15.8MB/s]"
          }
        },
        "507bba83cbc34446abad7a920c1c5e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e575b0c2a3ec4facb937eb436edf7bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e23f012d1e94f35b4b51e243b8a384c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40bc61745905499dbba6a03dddffa125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1751277d1e2847c1941b2a5406b71bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0da89bcece0542bebb304e345f41f1af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee38c5aa3aaa49fbbfbd8b3fd2d4ce64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f721be874179430d8972c535b685d3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ac009af6f3348a6afe2eaaa0d265275",
              "IPY_MODEL_680a2bec05e34f21b0f6d33807ef382f",
              "IPY_MODEL_5b94c044f02549e387ff8f0be4594c2b"
            ],
            "layout": "IPY_MODEL_eb9d08180c4a4240b8b1b5b806194892"
          }
        },
        "6ac009af6f3348a6afe2eaaa0d265275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8106c2126da64a2aa25a8b72731c4909",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8235a09671b340b1bec9a349ceb4a9f9",
            "value": "config.json:â€‡100%"
          }
        },
        "680a2bec05e34f21b0f6d33807ef382f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c730263b7e9454192ac8f1a870f5b03",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2f77d153d21490f8e56d1cb5908a040",
            "value": 665
          }
        },
        "5b94c044f02549e387ff8f0be4594c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06fea263db744e84a12119f6a0335d9f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e25305db5ea54e5ebdba117ae4e67cbb",
            "value": "â€‡665/665â€‡[00:00&lt;00:00,â€‡37.8kB/s]"
          }
        },
        "eb9d08180c4a4240b8b1b5b806194892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8106c2126da64a2aa25a8b72731c4909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8235a09671b340b1bec9a349ceb4a9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c730263b7e9454192ac8f1a870f5b03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2f77d153d21490f8e56d1cb5908a040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06fea263db744e84a12119f6a0335d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e25305db5ea54e5ebdba117ae4e67cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}