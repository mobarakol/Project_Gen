{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/Project_Gen/blob/main/SurgicalGPT_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvn6yKZCMHhU"
      },
      "source": [
        "Downlaod code and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p3dRC8LKXUN",
        "outputId": "601a8824-704b-48a7-fc1f-d216df570a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Project_Gen'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 97 (delta 53), reused 25 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (97/97), 43.32 KiB | 1.67 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "/content/Project_Gen/datasets\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "From (redirected): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN&confirm=t&uuid=bc46367c-c36a-4b57-b9d1-906a787f14a7\n",
            "To: /content/Project_Gen/datasets/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [01:03<00:00, 42.3MB/s]\n",
            "/content/Project_Gen\n"
          ]
        }
      ],
      "source": [
        "#Download code\n",
        "!git clone https://github.com/mobarakol/Project_Gen.git\n",
        "!mkdir /content/Project_Gen/datasets\n",
        "%cd /content/Project_Gen/datasets\n",
        "\n",
        "#Download Dataset\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip\n",
        "\n",
        "%cd /content/Project_Gen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install peft accelerate datasets"
      ],
      "metadata": {
        "id": "zOp7_2C1u0o6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03t52ES2wGu-",
        "outputId": "9ad2a9ba-4c47-45ea-c67a-b6946c4d2ab2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple/\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Project_Gen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y41z0K8x2INL",
        "outputId": "3d6ca343-b2e8-4685-f661-c4b82dadee3a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project_Gen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf vqa_datasets/pvqa"
      ],
      "metadata": {
        "id": "0JPftAh8BO74"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading PathVQA dataset:<br>\n",
        "(manually)"
      ],
      "metadata": {
        "id": "4cQvhKpJ6ZEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir vqa_datasets\n",
        "!gdown --id 1eDts_DGDrSRIGl7ajQZOip30cBP4J55x\n",
        "!mv pvqa.zip vqa_datasets\n",
        "!unzip -q vqa_datasets/pvqa.zip -d vqa_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3R25Oq738I2",
        "outputId": "5d5d148a-3503-44fe-e4a4-19c0233cda38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘vqa_datasets’: File exists\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1eDts_DGDrSRIGl7ajQZOip30cBP4J55x\n",
            "From (redirected): https://drive.google.com/uc?id=1eDts_DGDrSRIGl7ajQZOip30cBP4J55x&confirm=t&uuid=edd94f3b-4ba3-40ae-9bff-ec66ea2c7c38\n",
            "To: /content/Project_Gen/pvqa.zip\n",
            "100% 1.80G/1.80G [01:00<00:00, 29.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading PathVQA dataset:<br>\n",
        "(HF)"
      ],
      "metadata": {
        "id": "rT58FAhHBmnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARcFTcRQGc6A",
        "outputId": "21544263-0f82-41b7-b1d0-71d5fa9a6765"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m697.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import datasets\n",
        "datasets.config.DOWNLOADED_DATASETS_PATH = \"vqa_datasets\"\n",
        "dataset = load_dataset(\"flaviagiammarino/path-vqa\")\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']"
      ],
      "metadata": {
        "id": "Ofz8FID-1-PB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def update_classes(pkl_train, pkl_val, pkl_test):\n",
        "    # standardize answer ids across datasets and compute the maximum number of generated output tokens based on the train set\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    with open(pkl_train, 'rb') as f:\n",
        "            data_train = pickle.load(f)\n",
        "    with open(pkl_val, 'rb') as f:\n",
        "            data_val = pickle.load(f)\n",
        "    with open(pkl_test, 'rb') as f:\n",
        "            data_test = pickle.load(f)\n",
        "\n",
        "    cur_id = 0\n",
        "    class_names_list = []\n",
        "    class_ids_list = [[],[],[]]\n",
        "\n",
        "    for i, data in enumerate([data_train,data_val,data_test]):\n",
        "\n",
        "        for answer in data['answers']:\n",
        "            if answer not in class_names_list:\n",
        "                class_names_list.append(answer)\n",
        "                class_ids_list[i].append(cur_id)\n",
        "                cur_id+=1\n",
        "            else:\n",
        "                class_ids_list[i].append(class_names_list.index(answer))\n",
        "    q_lens = []\n",
        "    a_lens = []\n",
        "    for question in data_train['questions']:\n",
        "        q_lens.append(len(tokenizer.encode(question)))\n",
        "    for answer in data_train['answers']:\n",
        "        a_lens.append(len(tokenizer.encode(str(answer))))\n",
        "\n",
        "    data_train['class_ids'] = class_ids_list[0]\n",
        "    data_val['class_ids'] = class_ids_list[1]\n",
        "    data_test['class_ids'] = class_ids_list[2]\n",
        "\n",
        "    data_train['class_names'] = class_names_list\n",
        "    data_val['class_names'] = class_names_list\n",
        "    data_test['class_names'] = class_names_list\n",
        "\n",
        "    data_train['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "    data_val['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "    data_test['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "\n",
        "    with open(pkl_train, 'wb') as f:\n",
        "        pickle.dump(data_train,f)\n",
        "    with open(pkl_val, 'wb') as f:\n",
        "        pickle.dump(data_val,f)\n",
        "    with open(pkl_test, 'wb') as f:\n",
        "        pickle.dump(data_test,f)\n",
        "\n",
        "def preprocess_pathvqa(split, out_path):\n",
        "    device = torch.device('cuda:0')\n",
        "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "    data =  pd.read_pickle('vqa_datasets/pvqa/qas/{}/{}_qa.pkl'.format(split,split))\n",
        "    print(\"%0d captions loaded from json \" % len(data))\n",
        "    all_img_prefixes = []\n",
        "    img_ids = []\n",
        "    img_paths = []\n",
        "    all_questions = []\n",
        "    all_answers = []\n",
        "    img_dict = {}\n",
        "    for i in tqdm(range(len(data))):\n",
        "        d = data[i]\n",
        "        if d['answer']!=\"yes\" and d['answer']!=\"no\":\n",
        "            img_id = d[\"image\"]\n",
        "            filename = \"vqa_datasets/pvqa/images/{}/{}.jpg\".format(split,img_id)\n",
        "            with torch.no_grad():\n",
        "                prefix_i = clip_model.encode_image(preprocess(Image.open(filename)).unsqueeze(0).to(device)).cpu()\n",
        "            if img_id not in img_dict.keys():\n",
        "                img_dict[img_id] = [[d['question']],[d['answer']],prefix_i,filename]\n",
        "            else:\n",
        "                img_dict[img_id][0].append(d['question'])\n",
        "                img_dict[img_id][1].append(d['answer'])\n",
        "    # this dictionary is converted into a format that is sutiable for the data loader. Each data point contains a 'img_id', that corresponds is the index of the corresponding\n",
        "    # CLIP embedding of the image in 'img_prefix'.\n",
        "    for img_id, imgs in enumerate(img_dict.keys()):\n",
        "        all_img_prefixes.append(img_dict[imgs][3])\n",
        "        for q in range(len(img_dict[imgs][0])):\n",
        "            all_questions.append(img_dict[imgs][0][q])\n",
        "            all_answers.append(img_dict[imgs][1][q])\n",
        "            img_ids.append(img_id)\n",
        "            img_paths.append(img_dict[imgs][4])\n",
        "\n",
        "    all_data = {\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers,'img_path': img_paths}\n",
        "\n",
        "    with open(out_path, 'wb') as f:\n",
        "        pickle.dump(all_data,f)\n",
        "    print('Done')\n",
        "    print(\"%0d embeddings saved \" % len(all_img_prefixes))\n",
        "\n",
        "\n",
        "for split in ['train','val','test']:\n",
        "    out_path = \"vqa_datasets/pvqa/{}.pkl\".format(split)\n",
        "    preprocess_pathvqa(split,out_path)\n",
        "    update_classes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-E1SL5YDkgJ",
        "outputId": "fc5df1d3-da0a-4cd0-aff9-0f2b4547238e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19755 captions loaded from json \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 3442/19755 [00:36<03:19, 81.73it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart the session, please:"
      ],
      "metadata": {
        "id": "HvqeL8w6wp0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "import transformers\n",
        "from transformers import set_seed, GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers.models.biogpt import BioGptForCausalLM, BioGptTokenizer, BioGptConfig\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\n",
        "from peft import LoraConfig, get_peft_model,get_peft_config,PeftModelForCausalLM,TaskType,PrefixTuningConfig, PromptEncoderConfig, PromptTuningConfig\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(nn.Dropout(p=0.5))\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "class VQAmedModel(nn.Module):\n",
        "    def forward(self, prefix, labels, tokens, mask, q_len, batch_size):\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        if self.gpttype=='microsoft/biogpt':\n",
        "            embedding = self.gpt.transformer.embed_tokens(tokens)\n",
        "        else:\n",
        "            embedding = self.gpt.transformer.wte(tokens)\n",
        "        for b in range(batch_size):\n",
        "            # insert the visual prefix after the question\n",
        "            embedding[b,q_len[b]:q_len[b]+self.prefix_length,:] = prefix_projections[b]\n",
        "        return self.gpt(inputs_embeds=embedding, attention_mask=mask)\n",
        "    def generate(self, prefix, labels, tokens, mask, q_len):\n",
        "        prefix_projections = self.clip_project(prefix.view(1, -1)).view(self.prefix_length, self.gpt_embedding_size)\n",
        "        if self.gpttype=='microsoft/biogpt':\n",
        "            embedding_txt = self.gpt.transformer.embed_tokens(tokens)\n",
        "        else:\n",
        "            embedding_txt = self.gpt.transformer.wte(tokens)\n",
        "        embedding_txt[q_len:q_len+self.prefix_length,:] = prefix_projections\n",
        "        return embedding_txt\n",
        "    def __init__(\n",
        "        self,\n",
        "        prefix_length=2,\n",
        "        clip_length=2,\n",
        "        prefix_size=512,\n",
        "        num_layers=8,\n",
        "        setting=\"lora\",\n",
        "        mapping_type=\"MLP\",\n",
        "        args=None,\n",
        "    ):\n",
        "        super(VQAmedModel, self).__init__()\n",
        "        gpttype = args.model_type\n",
        "        self.gpttype = gpttype\n",
        "        self.setting = setting\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = AutoModelForCausalLM.from_pretrained(gpttype,load_in_8bit=True,device_map='auto')\n",
        "        # load the relevant fine-tuning strategy\n",
        "        if setting == \"lora\":\n",
        "            peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"prefixtuning\":\n",
        "            peft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"p_tuning\":\n",
        "            peft_config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"prompttuning\":\n",
        "            peft_config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting=='frozen':\n",
        "            for param in self.gpt.transformer.parameters():\n",
        "                param.requires_grad = False\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpttype)\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if mapping_type == \"MLP\":\n",
        "            self.clip_project = MLP((\n",
        "                    prefix_size,\n",
        "                    (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                    self.gpt_embedding_size * prefix_length,\n",
        "                    self.gpt_embedding_size * prefix_length))\n",
        "        # elif mapping_type == \"Transformer\":\n",
        "        #     self.clip_project = TransformerMapper(\n",
        "        #         prefix_size,\n",
        "        #         self.gpt_embedding_size,\n",
        "        #         prefix_length,\n",
        "        #         clip_length,\n",
        "        #         num_layers)\n",
        "        else:\n",
        "            raise ValueError(\"select valid mapping type: MLP or Transformer\")"
      ],
      "metadata": {
        "id": "RrdshczXrKUo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "parser.add_argument('--model_ver',      default= 'efvlegpt2rs18', )\n",
        "\n",
        "args = parser.parse_args([])\n",
        "\n",
        "\n",
        "model = VQAmedModel(\n",
        "            prefix_length=args.prefix_length,\n",
        "            clip_length=4,\n",
        "            setting=args.setting,\n",
        "            mapping_type=args.mapping_type,\n",
        "            args=args,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3btm9efh-ip",
        "outputId": "3804b005-5665-496f-fee4-47cf5c02d5c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed, GPT2Config, GPT2Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.biogpt import BioGptTokenizer\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "class medvqaDataset(Dataset):\n",
        "    def __init__(self, path, split='train',like_test=False,prefix_length=2,model_type = 'gpt2'):\n",
        "        super().__init__()\n",
        "        data_path = path+split+'.pkl'\n",
        "        with open(data_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        sys.stdout.flush()\n",
        "        self.model_type = model_type\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.img_ids = data[\"img_ids\"]\n",
        "        self.img_prefixes = data[\"img_prefix\"]\n",
        "        self.questions = data['questions']\n",
        "        self.answers = data['answers']\n",
        "        self.img_paths = data['img_path']\n",
        "\n",
        "        self.max_seqs_len = data['max_seqs_len']\n",
        "        self.labels = data['class_ids']\n",
        "        self.train_setting = True if (split!='test'and like_test==False) else False\n",
        "        self.prefix_len = prefix_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.answers)\n",
        "    def pad_sequences(self,index):\n",
        "        m = [torch.tensor(self.tokenizer.encode('question: ')),torch.tensor(self.tokenizer.encode(' context:')),torch.tensor(self.tokenizer.encode('answer ')),torch.tensor(self.tokenizer.encode('<|endoftext|>'))]\n",
        "        m_mask = [torch.ones(len(self.tokenizer.encode('question: '))),torch.ones(len(self.tokenizer.encode(' context:'))),torch.ones(len(self.tokenizer.encode('answer '))),torch.zeros(len(self.tokenizer.encode('<|endoftext|>')))]\n",
        "\n",
        "        if self.train_setting:\n",
        "            # construct the model input. The order is question, image, answer. During training the answer is masked. Any padding is placed on the right of the sequence.\n",
        "            # placeholder tokens are used on the location where the visual prefix will be inserted, with q_len indicating this location.\n",
        "            q=torch.tensor(self.tokenizer.encode(self.questions[index]))\n",
        "            a=torch.tensor(self.tokenizer.encode(str(self.answers[index])))\n",
        "\n",
        "            q,q_mask,leftover_tokens = self.make_padding(self.max_seqs_len[0],q,question=True)\n",
        "            q_len = m[0].size(0) + q.size(0) + m[1].size(0)\n",
        "            a,a_mask,_ = self.make_padding(self.max_seqs_len[1],a,leftover_tokens=leftover_tokens)\n",
        "            if len((a==0).nonzero())!=0:\n",
        "                pad_start = (a==0).nonzero()[0]\n",
        "            else:\n",
        "                pad_start=[]\n",
        "            a = torch.cat((a,m[3])) if len(pad_start)==0 else torch.cat((a[:pad_start],m[3],a[pad_start:]))\n",
        "            q = torch.cat((m[0],q,m[1],torch.ones(self.prefix_len),m[2],a))\n",
        "\n",
        "            q_mask = torch.cat((m_mask[0],q_mask,m_mask[1],torch.ones(self.prefix_len),m_mask[2],a_mask,m_mask[3]))\n",
        "            return q,q_mask, q_len\n",
        "        else:\n",
        "            # in the test stage we do not have acces to the answer, so we just load the question.\n",
        "            # since inference is not performed batch-wised we don't need to apply padding\n",
        "            q = torch.tensor(self.tokenizer.encode(self.questions[index]))\n",
        "\n",
        "            q,q_mask,_ = self.make_padding_test_setting(self.max_seqs_len[0],q)\n",
        "            q_len = m[0].size(0) + q.size(0) + m[1].size(0)\n",
        "            q = torch.cat((m[0],q,m[1],torch.ones(self.prefix_len),m[2]))\n",
        "\n",
        "\n",
        "            q_mask = torch.cat((m_mask[0],q_mask,m_mask[1]))\n",
        "            return q,q_mask,q_len\n",
        "\n",
        "    def make_padding(self, max_len, tokens, question=False,leftover_tokens=0):\n",
        "        padding = max_len - tokens.size(0)\n",
        "        if padding > 0:\n",
        "            if question:\n",
        "                leftover_tokens = padding\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "            else:\n",
        "                tokens = torch.cat((tokens, torch.zeros(padding+leftover_tokens)))\n",
        "                mask = torch.zeros(max_len+leftover_tokens)\n",
        "\n",
        "        elif padding==0:\n",
        "            if question:\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "            else:\n",
        "                mask = torch.zeros(tokens.size(0)+leftover_tokens)\n",
        "                tokens = torch.cat((tokens,torch.zeros(leftover_tokens)))\n",
        "\n",
        "\n",
        "        elif padding < 0:\n",
        "            if question:\n",
        "                tokens = tokens[:max_len]\n",
        "                mask = torch.ones(max_len)\n",
        "            else:\n",
        "                tokens = torch.cat((tokens[:max_len], torch.zeros(leftover_tokens)))\n",
        "                mask = torch.zeros(max_len+ leftover_tokens)\n",
        "        return tokens, mask, leftover_tokens\n",
        "    def make_padding_test_setting(self, max_len, tokens,do_padding=False):\n",
        "        padding = max_len - tokens.size(0)\n",
        "        padding_len = 0\n",
        "        if padding > 0:\n",
        "            if do_padding:\n",
        "                mask = torch.cat((torch.ones(tokens.size(0)),torch.zeros(padding)))\n",
        "                tokens = torch.cat((tokens,torch.zeros(padding)))\n",
        "                padding_len = padding\n",
        "            else:\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "        elif padding ==0:\n",
        "            mask = torch.ones(max_len)\n",
        "        elif padding < 0:\n",
        "            tokens = tokens[:max_len]\n",
        "            mask = torch.ones(max_len)\n",
        "        return tokens, mask, padding_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        prefix = self.img_prefixes[self.img_ids[index]]\n",
        "        tokens, mask, q_len  = self.pad_sequences(index)\n",
        "        return prefix,  self.labels[index], tokens, mask, q_len\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "parser.add_argument('--model_ver',      default= 'efvlegpt2rs18', )\n",
        "parser.add_argument(\"--dataset\", type=str, default='pathvqa', choices=('pathvqa', 'ovqa', 'slake'))\n",
        "parser.add_argument(\"--dataset_path\", type=str, default=\"vqa_datasets/\")\n",
        "\n",
        "args = parser.parse_args([])\n",
        "\n",
        "\n",
        "\n",
        "#suffix = f\"v5_{args.data_partition}_prefixlength_{args.prefix_length}_mapping_{args.mapping_type}_seed_{args.seed}_gpttype_{args.model_type.replace('/','')}_setting_{args.setting}_dataset_{args.dataset}\"\n",
        "train_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"train_img_id2idx\",prefix_length=args.prefix_length,model_type=args.model_type)#,abl=args.ablation)\n",
        "val_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"val_img_id2idx\",prefix_length=args.prefix_length,model_type=args.model_type)#, abl=args.ablation)\n",
        "test_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"test_img_id2idx\",prefix_length=args.prefix_length,model_type=args.model_type,like_test=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "41Bf8MRc3F28",
        "outputId": "fd372b64-cf29-4614-af2f-c99b7949e843"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'img_ids'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-be53fd945948>\u001b[0m in \u001b[0;36m<cell line: 142>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m#suffix = f\"v5_{args.data_partition}_prefixlength_{args.prefix_length}_mapping_{args.mapping_type}_seed_{args.seed}_gpttype_{args.model_type.replace('/','')}_setting_{args.setting}_dataset_{args.dataset}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedvqaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'pvqa/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_img_id2idx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,abl=args.ablation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedvqaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'pvqa/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_img_id2idx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, abl=args.ablation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedvqaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'pvqa/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_img_id2idx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlike_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-be53fd945948>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, split, like_test, prefix_length, model_type)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_prefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img_prefix\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'questions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'img_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data  import DataLoader\n",
        "\n",
        "class EndoVis18VQAGPTSentence(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head = '../dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa2/Sentence/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.Resize((300,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                q_s, an_s = line.split('|')\n",
        "                q_s = q_s.split('&')\n",
        "                an_s = an_s.split(('&'))\n",
        "                for i in range(len(q_s)):\n",
        "                    q_a = q_s[i]+'|'+an_s[i]\n",
        "                    # print(file, q_a)\n",
        "                    self.vqas.append([file, q_a])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img loc[3],\n",
        "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        answer = '<|sep|> '+answer\n",
        "\n",
        "\n",
        "        return img, question, answer\n",
        "\n",
        "\n",
        "# data location\n",
        "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "val_seq = [1, 5, 16]\n",
        "\n",
        "folder_head = 'datasets_my/EndoVis-18-VQA/seq_'\n",
        "folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "valid_loader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "print('training sample:', len(train_dataset), 'val sample:', len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8GMeoGEy5Hr",
        "outputId": "ca656db6-52c8-4cee-f7c4-6bf8b8f59d06"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "training sample: 10574 val sample: 3216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "import os\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "parser.add_argument('--model_ver',      default= 'efvlegpt2rs18', )\n",
        "\n",
        "\n",
        "args = parser.parse_args([])\n",
        "\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "if not os.path.exists(args.out_dir):\n",
        "    os.makedirs(args.out_dir)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=args.warmup_steps,\n",
        "    num_training_steps=args.epochs * len(train_loader),\n",
        ")\n",
        "\n",
        "valid_loader = accelerator.prepare(valid_loader)\n",
        "\n",
        "best_valid_loss = float(\"inf\")\n",
        "counter = 0\n",
        "n_epochs = args.epochs\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    with tqdm(total=args.batch_size * len(train_loader)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0.0\n",
        "        total_rocauc = 0.0\n",
        ""
      ],
      "metadata": {
        "id": "fV3_o0ScxDI6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2RoeUOhPErX"
      },
      "source": [
        "Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvA3l8GpPG7y"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLSnki7sPIfM"
      },
      "source": [
        "Running Script:<br>\n",
        "%cd /content/Project_Gen\n",
        "import os\n",
        "os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "!python train_SGPT_V2_Sentence.py \\\n",
        "--lr=0.00001 \\\n",
        "--checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_' \\\n",
        "--dataset_type='m18' \\\n",
        "--tokenizer_ver='gpt2v1' \\\n",
        "--model_ver='efvlegpt2rs18' \\\n",
        "--model_subver='v1' \\\n",
        "--vis_pos_emb='zeroes'\\\n",
        "--batch_size=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48GwkJQvPArS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ac10d4-0cb5-446d-fdb9-24b3bd8cbfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project_Gen\n",
            "device = cuda\n",
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "model params:  174607680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 139/265 [03:20<03:18,  1.57s/it]"
          ]
        }
      ],
      "source": [
        "%cd /content/Project_Gen\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch.backends.cudnn as cudnn\n",
        "# from model.EFGPT2Sentence import EFVLEGPT2RS18Sentence\n",
        "# from dataloader.dataloaderGPT2Sentence import EndoVis18VQAGPTSentence\n",
        "# from utils import AverageMeter, save_clf_checkpoint, adjust_learning_rate\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from transformers import  VisualBertConfig, GPT2Config\n",
        "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
        "from transformers import GPT2LMHeadModel\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import ViTFeatureExtractor, AutoFeatureExtractor\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def save_clf_checkpoint(checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, Acc):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'Acc': Acc,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer}\n",
        "    filename = checkpoint_dir + 'Best.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "\n",
        "class EndoVis18VQAGPTSentence(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head = '../dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa2/Sentence/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.Resize((300,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                q_s, an_s = line.split('|')\n",
        "                q_s = q_s.split('&')\n",
        "                an_s = an_s.split(('&'))\n",
        "                for i in range(len(q_s)):\n",
        "                    q_a = q_s[i]+'|'+an_s[i]\n",
        "                    # print(file, q_a)\n",
        "                    self.vqas.append([file, q_a])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img loc[3],\n",
        "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        answer = '<|sep|> '+answer\n",
        "\n",
        "\n",
        "        return img, question, answer\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class EFVLEGPT2RS18Sentence(nn.Module):\n",
        "    def __init__(self, model_subver = 'v3', tokenizer_len=50258, vis_pos_emb = None):\n",
        "        super(EFVLEGPT2RS18Sentence, self).__init__()\n",
        "        '''\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
        "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
        "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
        "        '''\n",
        "\n",
        "        self.sub_ver = model_subver\n",
        "        self.vis_pos_emb = vis_pos_emb\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
        "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "        self.img_feature_extractor.fc = new_fc\n",
        "        self.visual_embedder = nn.Linear(512, 768)\n",
        "\n",
        "        ## word_embedding default GPT2 word embedding\n",
        "        gpt2configuration = GPT2Config()\n",
        "        word_embedder = GPT2Model(gpt2configuration)\n",
        "        word_embedder.resize_token_embeddings(tokenizer_len)\n",
        "        self.word_embedder = word_embedder.wte\n",
        "\n",
        "        ## GPT2 visual context aware decoder\n",
        "        self.VCAdecoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "    def forward(self, question, img, answer):\n",
        "\n",
        "        ## image encoder features\n",
        "        img_feature = self.img_feature_extractor(img)\n",
        "        img_feature = torch.unsqueeze(img_feature, dim=1)\n",
        "\n",
        "        ## visual Embedding : id type 1, pos: zero / incremental\n",
        "        visual_embeds = self.visual_embedder(img_feature)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "        visual_attention_mask = visual_attention_mask.to(device)\n",
        "\n",
        "        ## question embedding:\n",
        "        question['input_ids'] = question['input_ids'].to(device)\n",
        "        question_embeds = self.word_embedder(question['input_ids'])\n",
        "        question_attention_mask = question['attention_mask'].to(device)\n",
        "\n",
        "        ## answer embedding\n",
        "        answer['input_ids'] = answer['input_ids'].to(device)\n",
        "        answer_embeds = self.word_embedder(answer['input_ids'])\n",
        "        answer_attention_mask = answer['attention_mask'].to(device)\n",
        "\n",
        "        ## token type of zero and position id for question\n",
        "        question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        question_position_id = torch.arange(0,question_embeds.size()[1])\n",
        "        question_position_id = torch.unsqueeze(question_position_id,0)\n",
        "        question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
        "        question_position_id = question_position_id.to(device)\n",
        "        question_len = len(question_position_id[0])\n",
        "        visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        visual_len = len(visual_position_id[0])\n",
        "        answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
        "        answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
        "        answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
        "        answer_position_id += (question_len+visual_len)\n",
        "        answer_position_id = answer_position_id.to(device)\n",
        "\n",
        "        ## question first\n",
        "        inputs_embeds = torch.cat((question_embeds, visual_embeds, answer_embeds), dim=1)\n",
        "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask, answer_attention_mask), dim=1)\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            token_type_ids = torch.cat((question_id_type, visual_id_type, answer_id_type), dim=1)\n",
        "            position_ids = torch.cat((question_position_id, visual_position_id, answer_position_id), dim=1)\n",
        "\n",
        "\n",
        "        ## VCA_GPT2 decoder\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
        "        else:\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, ( visual_features, questions, answers) in enumerate(tqdm(train_dataloader),0):\n",
        "\n",
        "        # prepare questions and answers\n",
        "        question_list = []\n",
        "        answer_list = []\n",
        "        for question in questions: question_list.append(question)\n",
        "        for answer in answers: answer_list.append(answer)\n",
        "\n",
        "        question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "        answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "        # Visual features\n",
        "        visual_features = visual_features.to(device)\n",
        "        visual_len = 80\n",
        "\n",
        "        # model forward(question, img, answer)\n",
        "        logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "        # only consider loss on reference summary just like seq2seq models\n",
        "        idx = args.question_len + 1\n",
        "\n",
        "        shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "        shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "        shift_labels = shift_labels.to(device)\n",
        "\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "\n",
        "    print(\"Epoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (visual_features, questions, answers) in enumerate(tqdm(val_loader),0):\n",
        "\n",
        "            # prepare questions and answers\n",
        "            question_list = []\n",
        "            answer_list = []\n",
        "            for question in questions: question_list.append(question)\n",
        "            for answer in answers: answer_list.append(answer)\n",
        "\n",
        "            if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "                answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = visual_features.to(device)\n",
        "                visual_len = 80\n",
        "\n",
        "            # model forward(question, img, answer)\n",
        "            logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "\n",
        "            # only consider loss on reference summary just like seq2seq models\n",
        "            idx = args.question_len + 1\n",
        "            shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "            shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "\n",
        "            # copy for logits and labels for sentence decoding and blue-4 score calculation\n",
        "            logits_copy = logits.clone()\n",
        "            shift_labels_copy = shift_labels.clone()\n",
        "\n",
        "            # loss calculation\n",
        "            shift_labels = shift_labels.to(device)\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # references    - Ground truth answer\n",
        "            answer_GT_dec = tokenizer.batch_decode(shift_labels_copy, skip_special_tokens= True)\n",
        "            for answer_GT_dec_i in answer_GT_dec: references.append([answer_GT_dec_i.split()])\n",
        "            # print(references)\n",
        "\n",
        "            # Hypotheses - predicted answer\n",
        "            _, answer_Gen_id = torch.max(logits_copy, dim=2)\n",
        "            answer_Gen_dec = tokenizer.batch_decode(answer_Gen_id, skip_special_tokens= True)\n",
        "            for answer_Gen_dec_i in answer_Gen_dec: hypotheses.append(answer_Gen_dec_i.split())\n",
        "            # print(hypotheses)\n",
        "\n",
        "\n",
        "        # Calculate BLEU1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "        print(\"Epoch: {}/{} EVA LOSS: {:.6f} BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
        "          (epoch, args.epochs, total_loss.avg, metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=50,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                            help=' 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2rs18/m18/v3_p_qf_',      help='m18/c80')\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                         help='m18/c80')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--model_subver',   default= 'v3',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--answer_len',     default= 35,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2rs18',                               help='efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--vis_pos_emb',    default= 'pos',                                         help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    args = get_arg()\n",
        "    args.lr = 0.00005\n",
        "    args.epochs = 2\n",
        "    args.checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_'\n",
        "    args.dataset_type='m18'\n",
        "    args.tokenizer_ver='gpt2v1'\n",
        "    args.model_ver='efvlegpt2rs18'\n",
        "    args.model_subver='v1'\n",
        "    args.vis_pos_emb='zeroes'\n",
        "    args.batch_size=40\n",
        "    os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer_length = len(tokenizer)\n",
        "\n",
        "    # data location\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = 'datasets/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "    val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "    model = EFVLEGPT2RS18Sentence(model_subver = args.model_subver, tokenizer_len=len(tokenizer), vis_pos_emb = args.vis_pos_emb)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    #evsluation\n",
        "    # checkpoint = torch.load(args.checkpoint, map_location=str(device))\n",
        "    # start_epoch = checkpoint['epoch']\n",
        "    # epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "    # model = checkpoint['model']\n",
        "    # optimizer = checkpoint['optimizer']\n",
        "    # final_args = checkpoint['final_args']\n",
        "    # for key in final_args.keys(): args.__setattr__(key, final_args[key])\n",
        "\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    model = model.to(device)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = CrossEntropyLoss().to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        # validation\n",
        "        metrics = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "            best_results[0] = metrics[\"Bleu_4\"]\n",
        "            best_epoch[0] = epoch\n",
        "            save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0])\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sph3wTEeJueY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}