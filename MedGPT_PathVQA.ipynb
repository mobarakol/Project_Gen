{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/Project_Gen/blob/main/MedGPT_PathVQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvn6yKZCMHhU"
      },
      "source": [
        "Downlaod code and dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install peft accelerate datasets\n",
        "!pip -q install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip -q install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "zOp7_2C1u0o6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1157af-54d0-4fee-dffa-a4f040773505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U evaluate bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlypc7GYvFar",
        "outputId": "a95770c3-0af7-4d22-f1c2-d1afd0ac957b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading PathVQA dataset:(manually)<br>\n",
        "Downloading PathVQA dataset:(HF)<br>\n",
        "```\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "datasets.config.DOWNLOADED_DATASETS_PATH = \"vqa_datasets\"\n",
        "dataset = load_dataset(\"flaviagiammarino/path-vqa\")\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']\n",
        "```"
      ],
      "metadata": {
        "id": "4cQvhKpJ6ZEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir vqa_datasets\n",
        "!gdown --id 1eDts_DGDrSRIGl7ajQZOip30cBP4J55x\n",
        "!mv pvqa.zip vqa_datasets\n",
        "!unzip -q vqa_datasets/pvqa.zip -d vqa_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3R25Oq738I2",
        "outputId": "b440f547-691c-4d86-ec22-4c674789a4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1eDts_DGDrSRIGl7ajQZOip30cBP4J55x\n",
            "From (redirected): https://drive.google.com/uc?id=1eDts_DGDrSRIGl7ajQZOip30cBP4J55x&confirm=t&uuid=d898381f-001a-42e4-8d74-9fb09ae32343\n",
            "To: /content/pvqa.zip\n",
            "100% 1.80G/1.80G [00:34<00:00, 51.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the PathVQA for GPT training:"
      ],
      "metadata": {
        "id": "FWugxS-gUXb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from transformers import GPT2Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "def update_classes(pkl_train, pkl_val, pkl_test):\n",
        "    # standardize answer ids across datasets and compute the maximum number of generated output tokens based on the train set\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    with open(pkl_train, 'rb') as f:\n",
        "            data_train = pickle.load(f)\n",
        "    with open(pkl_val, 'rb') as f:\n",
        "            data_val = pickle.load(f)\n",
        "    with open(pkl_test, 'rb') as f:\n",
        "            data_test = pickle.load(f)\n",
        "\n",
        "    cur_id = 0\n",
        "    class_names_list = []\n",
        "    class_ids_list = [[],[],[]]\n",
        "\n",
        "    for i, data in enumerate([data_train,data_val,data_test]):\n",
        "\n",
        "        for answer in data['answers']:\n",
        "            if answer not in class_names_list:\n",
        "                class_names_list.append(answer)\n",
        "                class_ids_list[i].append(cur_id)\n",
        "                cur_id+=1\n",
        "            else:\n",
        "                class_ids_list[i].append(class_names_list.index(answer))\n",
        "    q_lens = []\n",
        "    a_lens = []\n",
        "    for question in data_train['questions']:\n",
        "        q_lens.append(len(tokenizer.encode(question)))\n",
        "    for answer in data_train['answers']:\n",
        "        a_lens.append(len(tokenizer.encode(str(answer))))\n",
        "\n",
        "    data_train['class_ids'] = class_ids_list[0]\n",
        "    data_val['class_ids'] = class_ids_list[1]\n",
        "    data_test['class_ids'] = class_ids_list[2]\n",
        "\n",
        "    data_train['class_names'] = class_names_list\n",
        "    data_val['class_names'] = class_names_list\n",
        "    data_test['class_names'] = class_names_list\n",
        "\n",
        "    data_train['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "    data_val['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "    data_test['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
        "\n",
        "    with open(pkl_train, 'wb') as f:\n",
        "        pickle.dump(data_train,f)\n",
        "    with open(pkl_val, 'wb') as f:\n",
        "        pickle.dump(data_val,f)\n",
        "    with open(pkl_test, 'wb') as f:\n",
        "        pickle.dump(data_test,f)\n",
        "\n",
        "def preprocess_pathvqa(split, out_path):\n",
        "    device = torch.device('cuda:0')\n",
        "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "    data =  pd.read_pickle('vqa_datasets/pvqa/qas/{}/{}_qa.pkl'.format(split,split))\n",
        "    print(\"%0d captions loaded from json \" % len(data))\n",
        "    all_img_prefixes = []\n",
        "    img_ids = []\n",
        "    img_paths = []\n",
        "    all_questions = []\n",
        "    all_answers = []\n",
        "    img_dict = {}\n",
        "    for i in tqdm(range(len(data))):\n",
        "        d = data[i]\n",
        "        if d['answer']!=\"yes\" and d['answer']!=\"no\":\n",
        "            img_id = d[\"image\"]\n",
        "            filename = \"vqa_datasets/pvqa/images/{}/{}.jpg\".format(split,img_id)\n",
        "            with torch.no_grad():\n",
        "                prefix_i = clip_model.encode_image(preprocess(Image.open(filename)).unsqueeze(0).to(device)).cpu()\n",
        "            if img_id not in img_dict.keys():\n",
        "                img_dict[img_id] = [[d['question']],[d['answer']],prefix_i,filename]\n",
        "            else:\n",
        "                img_dict[img_id][0].append(d['question'])\n",
        "                img_dict[img_id][1].append(d['answer'])\n",
        "    # this dictionary is converted into a format that is sutiable for the data loader. Each data point contains a 'img_id', that corresponds is the index of the corresponding\n",
        "    # CLIP embedding of the image in 'img_prefix'.\n",
        "    for img_id, imgs in enumerate(img_dict.keys()):\n",
        "        all_img_prefixes.append(img_dict[imgs][2])\n",
        "        for q in range(len(img_dict[imgs][0])):\n",
        "            all_questions.append(img_dict[imgs][0][q])\n",
        "            all_answers.append(img_dict[imgs][1][q])\n",
        "            img_ids.append(img_id)\n",
        "            img_paths.append(img_dict[imgs][3])\n",
        "\n",
        "    all_data = {\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers,'img_path': img_paths}\n",
        "\n",
        "    with open(out_path, 'wb') as f:\n",
        "        pickle.dump(all_data,f)\n",
        "    print('Done')\n",
        "    print(\"%0d embeddings saved \" % len(all_img_prefixes))\n",
        "\n",
        "\n",
        "for split in ['train','val','test']:\n",
        "    out_path = \"vqa_datasets/pvqa/{}.pkl\".format(split)\n",
        "    preprocess_pathvqa(split,out_path)\n",
        "\n",
        "update_classes(\"vqa_datasets/pvqa/train.pkl\", \"vqa_datasets/pvqa/val.pkl\", \"vqa_datasets/pvqa/test.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "I-E1SL5YDkgJ",
        "outputId": "bd203a41-2430-42f9-a8ce-2a847f4ca9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19755 captions loaded from json \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 7931/19755 [01:44<02:35, 76.01it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d547e17f930e>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"vqa_datasets/pvqa/{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mpreprocess_pathvqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mupdate_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vqa_datasets/pvqa/train.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vqa_datasets/pvqa/val.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vqa_datasets/pvqa/test.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-d547e17f930e>\u001b[0m in \u001b[0;36mpreprocess_pathvqa\u001b[0;34m(split, out_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"vqa_datasets/pvqa/images/{}/{}.jpg\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprefix_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mimg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mencode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NLD -> LND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# LND -> NLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0morig_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         )\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training Process\n",
        "Dataloader:"
      ],
      "metadata": {
        "id": "qa84IGuYjm-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed, GPT2Config, GPT2Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.biogpt import BioGptTokenizer\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import pdb\n",
        "import argparse\n",
        "\n",
        "class medvqaDataset(Dataset):\n",
        "    def __init__(self, path, split='train',like_test=False,prefix_length=2,model_type = 'gpt2'):\n",
        "        super().__init__()\n",
        "        data_path = path+split+'.pkl'\n",
        "        with open(data_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        sys.stdout.flush()\n",
        "        self.model_type = model_type\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.img_ids = data[\"img_ids\"]\n",
        "        self.img_prefixes = data[\"img_prefix\"]\n",
        "        self.questions = data['questions']\n",
        "        self.answers = data['answers']\n",
        "        self.img_paths = data['img_path']\n",
        "\n",
        "        self.max_seqs_len = data['max_seqs_len']\n",
        "        self.labels = data['class_ids']\n",
        "        self.train_setting = True if (split!='test'and like_test==False) else False\n",
        "        self.prefix_len = prefix_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.answers)\n",
        "    def pad_sequences(self,index):\n",
        "        m = [torch.tensor(self.tokenizer.encode('question: ')),torch.tensor(self.tokenizer.encode(' context:')),torch.tensor(self.tokenizer.encode('answer ')),torch.tensor(self.tokenizer.encode('<|endoftext|>'))]\n",
        "        m_mask = [torch.ones(len(self.tokenizer.encode('question: '))),torch.ones(len(self.tokenizer.encode(' context:'))),torch.ones(len(self.tokenizer.encode('answer '))),torch.zeros(len(self.tokenizer.encode('<|endoftext|>')))]\n",
        "\n",
        "        if self.train_setting:\n",
        "            # construct the model input. The order is question, image, answer. During training the answer is masked. Any padding is placed on the right of the sequence.\n",
        "            # placeholder tokens are used on the location where the visual prefix will be inserted, with q_len indicating this location.\n",
        "            q=torch.tensor(self.tokenizer.encode(self.questions[index]))\n",
        "            a=torch.tensor(self.tokenizer.encode(str(self.answers[index])))\n",
        "\n",
        "            q,q_mask,leftover_tokens = self.make_padding(self.max_seqs_len[0],q,question=True)\n",
        "            q_len = m[0].size(0) + q.size(0) + m[1].size(0)\n",
        "            a,a_mask,_ = self.make_padding(self.max_seqs_len[1],a,leftover_tokens=leftover_tokens)\n",
        "            if len((a==0).nonzero())!=0:\n",
        "                pad_start = (a==0).nonzero()[0]\n",
        "            else:\n",
        "                pad_start=[]\n",
        "            a = torch.cat((a,m[3])) if len(pad_start)==0 else torch.cat((a[:pad_start],m[3],a[pad_start:]))\n",
        "            q = torch.cat((m[0],q,m[1],torch.ones(self.prefix_len),m[2],a))\n",
        "\n",
        "            q_mask = torch.cat((m_mask[0],q_mask,m_mask[1],torch.ones(self.prefix_len),m_mask[2],a_mask,m_mask[3]))\n",
        "            return q,q_mask, q_len\n",
        "        else:\n",
        "            # in the test stage we do not have acces to the answer, so we just load the question.\n",
        "            # since inference is not performed batch-wised we don't need to apply padding\n",
        "            q = torch.tensor(self.tokenizer.encode(self.questions[index]))\n",
        "\n",
        "            q,q_mask,_ = self.make_padding_test_setting(self.max_seqs_len[0],q)\n",
        "            q_len = m[0].size(0) + q.size(0) + m[1].size(0)\n",
        "            q = torch.cat((m[0],q,m[1],torch.ones(self.prefix_len),m[2]))\n",
        "\n",
        "\n",
        "            q_mask = torch.cat((m_mask[0],q_mask,m_mask[1]))\n",
        "            return q,q_mask,q_len\n",
        "\n",
        "    def make_padding(self, max_len, tokens, question=False,leftover_tokens=0):\n",
        "        padding = max_len - tokens.size(0)\n",
        "        if padding > 0:\n",
        "            if question:\n",
        "                leftover_tokens = padding\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "            else:\n",
        "                tokens = torch.cat((tokens, torch.zeros(padding+leftover_tokens)))\n",
        "                mask = torch.zeros(max_len+leftover_tokens)\n",
        "\n",
        "        elif padding==0:\n",
        "            if question:\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "            else:\n",
        "                mask = torch.zeros(tokens.size(0)+leftover_tokens)\n",
        "                tokens = torch.cat((tokens,torch.zeros(leftover_tokens)))\n",
        "\n",
        "\n",
        "        elif padding < 0:\n",
        "            if question:\n",
        "                tokens = tokens[:max_len]\n",
        "                mask = torch.ones(max_len)\n",
        "            else:\n",
        "                tokens = torch.cat((tokens[:max_len], torch.zeros(leftover_tokens)))\n",
        "                mask = torch.zeros(max_len+ leftover_tokens)\n",
        "        return tokens, mask, leftover_tokens\n",
        "    def make_padding_test_setting(self, max_len, tokens,do_padding=False):\n",
        "        padding = max_len - tokens.size(0)\n",
        "        padding_len = 0\n",
        "        if padding > 0:\n",
        "            if do_padding:\n",
        "                mask = torch.cat((torch.ones(tokens.size(0)),torch.zeros(padding)))\n",
        "                tokens = torch.cat((tokens,torch.zeros(padding)))\n",
        "                padding_len = padding\n",
        "            else:\n",
        "                mask = torch.ones(tokens.size(0))\n",
        "        elif padding ==0:\n",
        "            mask = torch.ones(max_len)\n",
        "        elif padding < 0:\n",
        "            tokens = tokens[:max_len]\n",
        "            mask = torch.ones(max_len)\n",
        "        return tokens, mask, padding_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        prefix = self.img_prefixes[self.img_ids[index]]\n",
        "        tokens, mask, q_len  = self.pad_sequences(index)\n",
        "        return prefix,  self.labels[index], tokens, mask, q_len\n",
        "\n"
      ],
      "metadata": {
        "id": "41Bf8MRc3F28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart the session, please:<br>\n",
        "training script"
      ],
      "metadata": {
        "id": "HvqeL8w6wp0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "import sys\n",
        "import transformers\n",
        "from transformers import set_seed, GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers.models.biogpt import BioGptForCausalLM, BioGptTokenizer, BioGptConfig\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\n",
        "from peft import LoraConfig, get_peft_model,get_peft_config,PeftModelForCausalLM,TaskType,PrefixTuningConfig, PromptEncoderConfig, PromptTuningConfig\n",
        "import random\n",
        "\n",
        "#eval\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "# from utils import generate_beam\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import GPT2Tokenizer\n",
        "import pdb\n",
        "from evaluate import load\n",
        "import collections\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(nn.Dropout(p=0.5))\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "class VQAmedModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        prefix_length=2,\n",
        "        clip_length=2,\n",
        "        prefix_size=512,\n",
        "        num_layers=8,\n",
        "        setting=\"lora\",\n",
        "        mapping_type=\"MLP\",\n",
        "        args=None,\n",
        "    ):\n",
        "        super(VQAmedModel, self).__init__()\n",
        "        gpttype = args.model_type\n",
        "        self.gpttype = gpttype\n",
        "        self.model_type = gpttype\n",
        "        self.setting = setting\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = AutoModelForCausalLM.from_pretrained(gpttype,load_in_8bit=True,device_map='auto')\n",
        "        # load the relevant fine-tuning strategy\n",
        "        if setting == \"lora\":\n",
        "            peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"prefixtuning\":\n",
        "            peft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"p_tuning\":\n",
        "            peft_config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting==\"prompttuning\":\n",
        "            peft_config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
        "            self.gpt = get_peft_model(self.gpt,peft_config)\n",
        "        elif setting=='frozen':\n",
        "            for param in self.gpt.transformer.parameters():\n",
        "                param.requires_grad = False\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpttype)\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if mapping_type == \"MLP\":\n",
        "            self.clip_project = MLP((\n",
        "                    prefix_size,\n",
        "                    (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                    self.gpt_embedding_size * prefix_length,\n",
        "                    self.gpt_embedding_size * prefix_length))\n",
        "        # elif mapping_type == \"Transformer\":\n",
        "        #     self.clip_project = TransformerMapper(\n",
        "        #         prefix_size,\n",
        "        #         self.gpt_embedding_size,\n",
        "        #         prefix_length,\n",
        "        #         clip_length,\n",
        "        #         num_layers)\n",
        "        else:\n",
        "            raise ValueError(\"select valid mapping type: MLP or Transformer\")\n",
        "\n",
        "    def forward(self, prefix, labels, tokens, mask, q_len, batch_size):\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        print('Mobarak tokens:', tokens.shape)\n",
        "        if self.gpttype=='microsoft/biogpt':\n",
        "            embedding = self.gpt.transformer.embed_tokens(tokens)\n",
        "        else:\n",
        "            embedding = self.gpt.transformer.wte(tokens)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # insert the visual prefix after the question\n",
        "            embedding[b,q_len[b]:q_len[b]+self.prefix_length,:] = prefix_projections[b]\n",
        "            if b == 0:\n",
        "                print('mobarak embedding:', embedding.shape)\n",
        "        return self.gpt(inputs_embeds=embedding, attention_mask=mask)\n",
        "    def generate(self, prefix, labels, tokens, mask, q_len):\n",
        "        prefix_projections = self.clip_project(prefix.view(1, -1)).view(self.prefix_length, self.gpt_embedding_size)\n",
        "        if self.gpttype=='microsoft/biogpt':\n",
        "            embedding_txt = self.gpt.transformer.embed_tokens(tokens)\n",
        "        else:\n",
        "            embedding_txt = self.gpt.transformer.wte(tokens)\n",
        "        embedding_txt[q_len:q_len+self.prefix_length,:] = prefix_projections\n",
        "        return embedding_txt\n",
        "\n",
        "\n",
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "def treebank_tokenize(s):\n",
        "    return TreebankWordTokenizer().tokenize(s)\n",
        "def generate_beam(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    beam_size: int = 5,\n",
        "    generated=None,\n",
        "    entry_length=65,\n",
        "    temperature=1.0,\n",
        "    stop_token: str = \"<|endoftext|>\",\n",
        "):\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "            logits = logits.softmax(-1).log()\n",
        "            # final_logit\n",
        "\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(\n",
        "                    beam_size, -1\n",
        "                )\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            if model.model_type == \"biogpt\":\n",
        "                next_token_embed = model.gpt.biogpt.embed_tokens(\n",
        "                    next_tokens.squeeze()\n",
        "                ).view(generated.shape[0], 1, -1)\n",
        "            elif model.model_type == \"gpt2\":\n",
        "                next_token_embed = model.gpt.transformer.wte(\n",
        "                    next_tokens.squeeze()\n",
        "                ).view(generated.shape[0], 1, -1)\n",
        "            else:\n",
        "                next_token_embed = model.gpt.get_input_embeddings()(tokens[:,-1])\n",
        "                next_token_embed=next_token_embed.squeeze().view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [\n",
        "        tokenizer.decode(output[: int(length)])\n",
        "        for output, length in zip(output_list, seq_lengths)\n",
        "    ]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "# from utils import generate_beam\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import GPT2Tokenizer\n",
        "import pdb\n",
        "from evaluate import load\n",
        "import collections\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "\n",
        "def eval_gpt_open_ended(model, dataset, args, print_vis_token_meaning=False):\n",
        "    model.eval()\n",
        "    model=model.cuda()\n",
        "    bert_score = load(\"bertscore\")\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model.model_type)\n",
        "    bleu_avg1=0.\n",
        "    bert_avg1 = 0.\n",
        "    bert_avg2 = 0.\n",
        "    bert_avg3 = 0.\n",
        "    f1_avg = 0.\n",
        "    acc = 0.\n",
        "    acc_oe = 0.\n",
        "    acc_yn = 0.\n",
        "    c_oe =1e-9\n",
        "    c_yn =1e-9\n",
        "    with tqdm(total=len(dataset)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(\"Testing\")\n",
        "        for item in range(len(dataset)):\n",
        "            prefix,  labels, tokens, mask, q_len = dataset[item]\n",
        "            prefix = prefix.type(torch.float32).cuda()\n",
        "            tokens = tokens.type(torch.long).cuda()\n",
        "            mask = mask.cuda()\n",
        "            with autocast(dtype=torch.float16):\n",
        "              with torch.no_grad():\n",
        "                  embed = model.generate(prefix,labels,tokens,mask,q_len).view(1,tokens.size(0),-1)\n",
        "                  if print_vis_token_meaning:\n",
        "                    prefix_projections = embed[:,q_len:q_len+model.prefix_length,:]\n",
        "                    for i in range(prefix_projections.size(1)):\n",
        "                      print_nearest_text_token(prefix_projections[0,i], model)\n",
        "                  out_text = generate_beam(model, model.tokenizer,generated=embed,entry_length=dataset.max_seqs_len[1], temperature=1)[0]\n",
        "\n",
        "            if out_text.lower()==dataset.answers[item].lower():\n",
        "              acc+=1\n",
        "            if dataset.answers[item].lower()=='yes' or dataset.answers[item].lower()=='no':\n",
        "              if out_text.lower()==dataset.answers[item].lower():\n",
        "                acc_yn+=1\n",
        "              c_yn+=1\n",
        "            else:\n",
        "              if out_text.lower()==dataset.answers[item].lower():\n",
        "                acc_oe+=1\n",
        "              c_oe+=1\n",
        "\n",
        "            reference = [str(dataset.answers[item])]\n",
        "            candidate = [out_text]\n",
        "\n",
        "            bleu_1 = sentence_bleu(reference[0], candidate[0], weights=(1, 0, 0, 0))\n",
        "\n",
        "            a = bert_score.compute(references = reference,predictions = candidate,model_type = 'bert-base-uncased')\n",
        "            bert_avg1+= a['precision'][0]\n",
        "            bert_avg2+= a['recall'][0]\n",
        "            bert_avg3+= a['f1'][0]\n",
        "\n",
        "\n",
        "            f1_avg += compute_f1(tokenizer.encode(reference[0]),tokenizer.encode(candidate[0]))\n",
        "            bleu_avg1+=bleu_1\n",
        "\n",
        "\n",
        "    print('------------')\n",
        "    print(\"BLEU {}\".format(round(bleu_avg1/len(dataset),3)))\n",
        "    print(\"BERTScore {}\".format(round(bert_avg3/len(dataset),3)))\n",
        "    print(\"F1 {}\".format(round(f1_avg/len(dataset),3)))\n",
        "    print(\"Accuracy {}\".format(round(acc/len(dataset),3)))\n",
        "    print(\"Accuracy YN{}\".format(round(acc_yn/c_yn,3)))\n",
        "    print(\"Accuracy OE{}\".format(round(acc_oe/c_oe,3)))\n",
        "\n",
        "def print_nearest_text_token(vis_token, model):\n",
        "    \"\"\"print the nearest token in the vocabulary to the given token through model.gpt.embeddings.weight\"\"\"\n",
        "    embeddings = model.gpt.transformer.wte.weight\n",
        "    distances = torch.norm(embeddings - vis_token, dim=1)\n",
        "    nearest_token_idx = torch.argmin(distances)\n",
        "    print(model.tokenizer.decode([nearest_token_idx.item()]))\n",
        "\n",
        "def compute_f1(gold_toks, pred_toks):\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "\n",
        "def pytorch_model_run(train_loader, valid_loader, test_dataset, model_obj, args):\n",
        "    accelerator = Accelerator()\n",
        "    device = accelerator.device\n",
        "\n",
        "    if not os.path.exists(args.out_dir):\n",
        "        os.makedirs(args.out_dir)\n",
        "\n",
        "    model = model_obj.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=args.warmup_steps,\n",
        "        num_training_steps=args.epochs * len(train_loader),\n",
        "    )\n",
        "\n",
        "    ## 💡 introduce all components to accelerate library\n",
        "    model, optimizer, train_loader, scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_loader, scheduler\n",
        "    )\n",
        "    valid_loader = accelerator.prepare(valid_loader)\n",
        "\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    counter = 0\n",
        "    n_epochs = args.epochs\n",
        "    accelerator.wait_for_everyone()\n",
        "    for epoch in range(args.epochs):\n",
        "        model.to(device)\n",
        "        with tqdm(total=args.batch_size * len(train_loader)) as epoch_pbar:\n",
        "            epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
        "            start_time = time.time()\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "\n",
        "            for i, (prefix, labels, tokens, mask, q_len) in enumerate(train_loader):\n",
        "                with accelerator.accumulate(model):\n",
        "                    prefix = prefix.type(torch.float32)\n",
        "                    tokens = tokens.type(torch.long)\n",
        "                    mask = mask.type(torch.long)\n",
        "                    q_len = q_len.type(torch.long)\n",
        "                    outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                    logits = outputs.logits\n",
        "                    loss = 0.\n",
        "\n",
        "                    shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "\n",
        "                    for b in range(logits.size(0)):\n",
        "                        condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                        condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "\n",
        "                        loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                    loss=loss/logits.size(0)\n",
        "\n",
        "                    accelerator.backward(loss)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    total_loss += loss.item()\n",
        "                    avg_loss = total_loss / (i+1)\n",
        "                    desc = f\"Epoch {epoch} - loss {avg_loss:.20f}\"\n",
        "                    epoch_pbar.set_description(desc)\n",
        "                    epoch_pbar.update(prefix.shape[0])\n",
        "                    break\n",
        "                break\n",
        "        break\n",
        "        model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        with tqdm(total=args.batch_size * len(valid_loader)) as epoch_pbar:\n",
        "            epoch_pbar.set_description(f\"VAL Epoch {epoch}\")\n",
        "            for i, (prefix, labels, tokens, mask,q_len) in enumerate(valid_loader):\n",
        "                torch.cuda.empty_cache()\n",
        "                prefix = prefix.type(torch.float32)\n",
        "                tokens = tokens.type(torch.long)\n",
        "                mask = mask.type(torch.long)\n",
        "                q_len = q_len.type(torch.long)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                    logits = outputs.logits\n",
        "                    loss = 0.\n",
        "                    shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "                    for b in range(logits.size(0)):\n",
        "                        condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                        condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "                        loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                    loss=loss/logits.size(0)\n",
        "                    total_loss += loss.item()\n",
        "                avg_val_loss = total_loss / (i + 1)\n",
        "                desc = f\"VAL Epoch {epoch} - loss {avg_val_loss:.20f}\"\n",
        "                epoch_pbar.set_description(desc)\n",
        "                epoch_pbar.update(prefix.shape[0])\n",
        "                break\n",
        "\n",
        "        if avg_val_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_val_loss\n",
        "\n",
        "            torch.save(model.state_dict(), os.path.join(args.out_dir, f\"open_ended_latest.pt\"))\n",
        "\n",
        "        scheduler.step()\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(\n",
        "            \"VAL epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s\".format(\n",
        "                epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time\n",
        "            )\n",
        "        )\n",
        "        if avg_val_loss > avg_loss:\n",
        "            counter += 1\n",
        "        if counter == 5:\n",
        "            break\n",
        "\n",
        "        # eval_gpt_open_ended(model, test_dataset, args, print_vis_token_meaning=False)\n",
        "    return model"
      ],
      "metadata": {
        "id": "RrdshczXrKUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.nn.functional as nnf\n",
        "from accelerate import Accelerator\n",
        "import os\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "def parse_argument():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "    parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "    parser.add_argument(\"--ablation\", type=str, default=\"none\", choices=(\"remove_question\", \"remove_visual\",'replace_visual',\"swap\"))\n",
        "    parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "    parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "    parser.add_argument(\"--dataset_path\", type=str, default=\"vqa_datasets/\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=20)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "    parser.add_argument(\"--dataset\", type=str, default='pathvqa', choices=('pathvqa', 'ovqa', 'slake'))\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "    parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--iters_to_accumulate\", type=int, default=4)\n",
        "    parser.add_argument(\"--validation_step\", type=int, default=1000)\n",
        "    parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "    parser.add_argument(\"--checkpoint\", type=str)\n",
        "    parser.add_argument(\"--eval\", dest=\"eval\", action=\"store_true\")\n",
        "    parser.add_argument(\"--verbose\", dest=\"verbose\", action=\"store_true\")\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    set_random_seeds(args.seed)\n",
        "    return args\n",
        "\n",
        "args = parse_argument()\n",
        "\n",
        "suffix = f\"v5_prefixlength_{args.prefix_length}_mapping_{args.mapping_type}_seed_{args.seed}_gpttype_{args.model_type.replace('/','')}_setting_{args.setting}_dataset_{args.dataset}\"\n",
        "\n",
        "args.out_dir = os.path.join('checkpoints', suffix)\n",
        "\n",
        "train_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"train\",prefix_length=args.prefix_length,model_type=args.model_type)#,abl=args.ablation)\n",
        "val_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"val\",prefix_length=args.prefix_length,model_type=args.model_type)#, abl=args.ablation)\n",
        "test_dataset = medvqaDataset(args.dataset_path+'pvqa/',split=\"test\",prefix_length=args.prefix_length,model_type=args.model_type,like_test=True)\n",
        "print('Sample: Tain={}, valid={}, Test={}'.format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "model = VQAmedModel(\n",
        "            prefix_length=args.prefix_length,\n",
        "            clip_length=4,\n",
        "            setting=args.setting,\n",
        "            mapping_type=args.mapping_type,\n",
        "            args=args,\n",
        "        )\n",
        "\n",
        "model = pytorch_model_run(train_dataloader, val_dataloader, test_dataset, model, args)\n"
      ],
      "metadata": {
        "id": "EZMBItk-oR2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b904fb58-f718-4fae-f1ba-e822426bbf2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample: Tain=9949, valid=3144, Test=3370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 0:   0%|          | 0/9940 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mobarak tokens: torch.Size([20, 46])\n",
            "mobarak embedding: torch.Size([20, 46, 1600])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0 - loss 8.84375000000000000000:   0%|          | 20/9940 [00:00<07:57, 20.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset),len(train_dataloader), 9949/20"
      ],
      "metadata": {
        "id": "5YCCP3b6iiis",
        "outputId": "2253d9da-32cb-4187-c3e2-7af1ec7d384e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9949, 497, 497.45)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pytorch_model_run(train_loader, valid_loader, test_dataset, model_obj, args):\n",
        "    accelerator = Accelerator()\n",
        "    device = accelerator.device\n",
        "\n",
        "    if not os.path.exists(args.out_dir):\n",
        "        os.makedirs(args.out_dir)\n",
        "\n",
        "    model = model_obj.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=args.warmup_steps,\n",
        "        num_training_steps=args.epochs * len(train_loader),\n",
        "    )\n",
        "\n",
        "    ## 💡 introduce all components to accelerate library\n",
        "    model, optimizer, train_loader, scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_loader, scheduler\n",
        "    )\n",
        "    valid_loader = accelerator.prepare(valid_loader)\n",
        "\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    counter = 0\n",
        "    n_epochs = args.epochs\n",
        "    accelerator.wait_for_everyone()\n",
        "    for epoch in range(args.epochs):\n",
        "        model.to(device)\n",
        "        with tqdm(total=args.batch_size * len(train_loader)) as epoch_pbar:\n",
        "            epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
        "            start_time = time.time()\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "\n",
        "            for i, (prefix, labels, tokens, mask, q_len) in enumerate(train_loader):\n",
        "                with accelerator.accumulate(model):\n",
        "                    prefix = prefix.type(torch.float32)\n",
        "                    print('prefix:', prefix.shape, 'q_len:', q_len)\n",
        "                    tokens = tokens.type(torch.long)\n",
        "                    mask = mask.type(torch.long)\n",
        "                    q_len = q_len.type(torch.long)\n",
        "                    outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                    logits = outputs.logits\n",
        "                    print('logits:', logits.shape)\n",
        "                    loss = 0.\n",
        "\n",
        "                    shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "                    print('shift:', shift)\n",
        "                    print('model.prefix_length:', model.prefix_length)\n",
        "                    for b in range(logits.size(0)):\n",
        "                        condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                        condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "                        print('condensed_tokens:', condensed_tokens.shape)\n",
        "                        print('condensed_logits:', condensed_logits.shape)\n",
        "\n",
        "                        loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                        break\n",
        "                    loss=loss/logits.size(0)\n",
        "\n",
        "                    accelerator.backward(loss)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    total_loss += loss.item()\n",
        "                    avg_loss = total_loss / (i+1)\n",
        "                    desc = f\"Epoch {epoch} - loss {avg_loss:.20f}\"\n",
        "                    epoch_pbar.set_description(desc)\n",
        "                    epoch_pbar.update(prefix.shape[0])\n",
        "                    break\n",
        "                break\n",
        "        break\n",
        "        model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        with tqdm(total=args.batch_size * len(valid_loader)) as epoch_pbar:\n",
        "            epoch_pbar.set_description(f\"VAL Epoch {epoch}\")\n",
        "            for i, (prefix, labels, tokens, mask,q_len) in enumerate(valid_loader):\n",
        "                torch.cuda.empty_cache()\n",
        "                prefix = prefix.type(torch.float32)\n",
        "                tokens = tokens.type(torch.long)\n",
        "                mask = mask.type(torch.long)\n",
        "                q_len = q_len.type(torch.long)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                    logits = outputs.logits\n",
        "                    loss = 0.\n",
        "                    shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "                    for b in range(logits.size(0)):\n",
        "                        condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                        condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "                        loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                    loss=loss/logits.size(0)\n",
        "                    total_loss += loss.item()\n",
        "                avg_val_loss = total_loss / (i + 1)\n",
        "                desc = f\"VAL Epoch {epoch} - loss {avg_val_loss:.20f}\"\n",
        "                epoch_pbar.set_description(desc)\n",
        "                epoch_pbar.update(prefix.shape[0])\n",
        "                break\n",
        "\n",
        "        if avg_val_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_val_loss\n",
        "\n",
        "            torch.save(model.state_dict(), os.path.join(args.out_dir, f\"open_ended_latest.pt\"))\n",
        "\n",
        "        scheduler.step()\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(\n",
        "            \"VAL epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s\".format(\n",
        "                epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time\n",
        "            )\n",
        "        )\n",
        "        if avg_val_loss > avg_loss:\n",
        "            counter += 1\n",
        "        if counter == 5:\n",
        "            break\n",
        "\n",
        "        # eval_gpt_open_ended(model, test_dataset, args, print_vis_token_meaning=False)\n",
        "    return model\n",
        "\n",
        "model = pytorch_model_run(train_dataloader, val_dataloader, test_dataset, model, args)"
      ],
      "metadata": {
        "id": "z4-9hcZihUJD",
        "outputId": "4cc6691a-d123-492b-eed6-37d680d9812c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:   0%|          | 0/9940 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prefix: torch.Size([20, 512]) q_len: tensor([ 9,  9, 10, 11, 22,  9, 11,  9, 14,  9, 22, 11,  9,  9,  9,  9, 13,  9,\n",
            "         9, 11], device='cuda:0')\n",
            "mobarak embedding: torch.Size([20, 46, 1600])\n",
            "logits: torch.Size([20, 46, 50257])\n",
            "shift: 0\n",
            "model.prefix_length: 8\n",
            "condensed_tokens: torch.Size([28])\n",
            "condensed_logits: torch.Size([28, 50257])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0 - loss 0.44970703125000000000:   0%|          | 20/9940 [00:00<06:09, 26.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "46-19"
      ],
      "metadata": {
        "id": "a6Emv-rilshG",
        "outputId": "f67f4adf-3b6d-40c4-e0a6-18a52e468e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "uvlPX_083ISt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "def treebank_tokenize(s):\n",
        "    return TreebankWordTokenizer().tokenize(s)\n",
        "def generate_beam(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    beam_size: int = 5,\n",
        "    generated=None,\n",
        "    entry_length=65,\n",
        "    temperature=1.0,\n",
        "    stop_token: str = \"<|endoftext|>\",\n",
        "):\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "            print('logits:', logits.shape)\n",
        "\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "            print('logits2:', logits.shape)\n",
        "\n",
        "            logits = logits.softmax(-1).log()\n",
        "            print('logits3:', logits.shape)\n",
        "\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(\n",
        "                    beam_size, -1\n",
        "                )\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            if model.model_type == \"biogpt\":\n",
        "                next_token_embed = model.gpt.biogpt.embed_tokens(\n",
        "                    next_tokens.squeeze()\n",
        "                ).view(generated.shape[0], 1, -1)\n",
        "            elif model.model_type == \"gpt2\":\n",
        "                next_token_embed = model.gpt.transformer.wte(\n",
        "                    next_tokens.squeeze()\n",
        "                ).view(generated.shape[0], 1, -1)\n",
        "            else:\n",
        "                next_token_embed = model.gpt.get_input_embeddings()(tokens[:,-1])\n",
        "                next_token_embed=next_token_embed.squeeze().view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [\n",
        "        tokenizer.decode(output[: int(length)])\n",
        "        for output, length in zip(output_list, seq_lengths)\n",
        "    ]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "def eval_gpt_open_ended(model, dataset, args, print_vis_token_meaning=False):\n",
        "    model.eval()\n",
        "    model=model.cuda()\n",
        "    bert_score = load(\"bertscore\")\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model.model_type)\n",
        "    bleu_avg1=0.\n",
        "    bert_avg1 = 0.\n",
        "    bert_avg2 = 0.\n",
        "    bert_avg3 = 0.\n",
        "    f1_avg = 0.\n",
        "    acc = 0.\n",
        "    acc_oe = 0.\n",
        "    acc_yn = 0.\n",
        "    c_oe =1e-9\n",
        "    c_yn =1e-9\n",
        "    with tqdm(total=len(dataset)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(\"Testing\")\n",
        "        for item in range(len(dataset)):\n",
        "            prefix,  labels, tokens, mask, q_len = dataset[item]\n",
        "            prefix = prefix.type(torch.float32).cuda()\n",
        "            tokens = tokens.type(torch.long).cuda()\n",
        "            mask = mask.cuda()\n",
        "            with autocast(dtype=torch.float16):\n",
        "              with torch.no_grad():\n",
        "                  embed = model.generate(prefix,labels,tokens,mask,q_len).view(1,tokens.size(0),-1)\n",
        "                  if print_vis_token_meaning:\n",
        "                    prefix_projections = embed[:,q_len:q_len+model.prefix_length,:]\n",
        "                    for i in range(prefix_projections.size(1)):\n",
        "                      print_nearest_text_token(prefix_projections[0,i], model)\n",
        "                  out_text = generate_beam(model, model.tokenizer,generated=embed,entry_length=dataset.max_seqs_len[1], temperature=1)[0]\n",
        "\n",
        "            if out_text.lower()==dataset.answers[item].lower():\n",
        "              acc+=1\n",
        "            if dataset.answers[item].lower()=='yes' or dataset.answers[item].lower()=='no':\n",
        "              if out_text.lower()==dataset.answers[item].lower():\n",
        "                acc_yn+=1\n",
        "              c_yn+=1\n",
        "            else:\n",
        "              if out_text.lower()==dataset.answers[item].lower():\n",
        "                acc_oe+=1\n",
        "              c_oe+=1\n",
        "\n",
        "            reference = [str(dataset.answers[item])]\n",
        "            candidate = [out_text]\n",
        "\n",
        "            bleu_1 = sentence_bleu(reference[0], candidate[0], weights=(1, 0, 0, 0))\n",
        "\n",
        "            a = bert_score.compute(references = reference,predictions = candidate,model_type = 'bert-base-uncased')\n",
        "            bert_avg1+= a['precision'][0]\n",
        "            bert_avg2+= a['recall'][0]\n",
        "            bert_avg3+= a['f1'][0]\n",
        "\n",
        "\n",
        "            f1_avg += compute_f1(tokenizer.encode(reference[0]),tokenizer.encode(candidate[0]))\n",
        "            bleu_avg1+=bleu_1\n",
        "            break\n",
        "\n",
        "\n",
        "    print('------------')\n",
        "    print(\"BLEU {}\".format(round(bleu_avg1/len(dataset),3)))\n",
        "    print(\"BERTScore {}\".format(round(bert_avg3/len(dataset),3)))\n",
        "    print(\"F1 {}\".format(round(f1_avg/len(dataset),3)))\n",
        "    print(\"Accuracy {}\".format(round(acc/len(dataset),3)))\n",
        "    print(\"Accuracy YN{}\".format(round(acc_yn/c_yn,3)))\n",
        "    print(\"Accuracy OE{}\".format(round(acc_oe/c_oe,3)))\n",
        "\n",
        "# model = VQAmedModel(\n",
        "#             prefix_length=args.prefix_length,\n",
        "#             clip_length=4,\n",
        "#             setting=args.setting,\n",
        "#             mapping_type=args.mapping_type,\n",
        "#             args=args,\n",
        "#         )\n",
        "\n",
        "# checkpoint = os.path.join(args.out_dir, f\"open_ended_latest.pt\")\n",
        "# if args.verbose:\n",
        "#     print(f\">> Loading pre-trained model {checkpoint}!\")\n",
        "# if os.path.exists(checkpoint):\n",
        "#     print('ckpt exist')\n",
        "#     model.cuda()\n",
        "#     model.load_state_dict(\n",
        "#         torch.load(checkpoint, map_location=torch.device(\"cuda\")), strict=False\n",
        "#     )\n",
        "# else:\n",
        "#     raise ValueError(\"Please provide valid path for loading checkpoint\")\n",
        "\n",
        "model.cuda()\n",
        "eval_gpt_open_ended(model, test_dataset, args, print_vis_token_meaning=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSa2eVmOzcNy",
        "outputId": "7bfb93ee-43dd-426c-ff75-a9a9ee99f7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:   0%|          | 0/3370 [00:00<?, ?it/s]<ipython-input-27-24a67054ab1b>:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(dtype=torch.float16):\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits: torch.Size([1, 32, 50257])\n",
            "logits2: torch.Size([1, 50257])\n",
            "logits3: torch.Size([1, 50257])\n",
            "logits: torch.Size([5, 33, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 34, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 35, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 36, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 37, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 38, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 39, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 40, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 41, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 42, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 43, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n",
            "logits: torch.Size([5, 44, 50257])\n",
            "logits2: torch.Size([5, 50257])\n",
            "logits3: torch.Size([5, 50257])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTesting:   0%|          | 0/3370 [00:02<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "BLEU 0.0\n",
            "BERTScore 0.0\n",
            "F1 0.0\n",
            "Accuracy 0.0\n",
            "Accuracy YN0.0\n",
            "Accuracy OE0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "parser.add_argument('--model_ver',      default= 'efvlegpt2rs18', )\n",
        "parser.add_argument(\"--dataset\", type=str, default='pathvqa', choices=('pathvqa', 'ovqa', 'slake'))\n",
        "parser.add_argument(\"--dataset_path\", type=str, default=\"vqa_datasets/\")\n",
        "args = parser.parse_args([])\n",
        "\n",
        "\n",
        "model = VQAmedModel(\n",
        "            prefix_length=args.prefix_length,\n",
        "            clip_length=4,\n",
        "            setting=args.setting,\n",
        "            mapping_type=args.mapping_type,\n",
        "            args=args,\n",
        "        )\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3btm9efh-ip",
        "outputId": "28ffd897-30dd-43ba-9c14-ceaf65364152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.nn.functional as nnf\n",
        "from accelerate import Accelerator\n",
        "import os\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_type\", type=str, default=\"gpt2-xl\", choices=(\"gpt2-xl\", \"microsoft/biogpt\",\"stanford-crfm/BioMedLM\"))\n",
        "parser.add_argument(\"--prefix_length\", type=int, default=8)\n",
        "parser.add_argument(\"--setting\", type=str, default=\"frozen\", choices=(\"lora\", \"frozen\",'prefixtuning',\"p_tuning\",\"prompttuning\", \"unfrozen\"))\n",
        "parser.add_argument(\"--mapping_type\", type=str, default=\"MLP\")\n",
        "parser.add_argument(\"--out_dir\", default=\"checkpoints\")\n",
        "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=600)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "parser.add_argument(\"--epochs\", type=int, default=30)\n",
        "parser.add_argument('--model_ver',      default= 'efvlegpt2rs18', )\n",
        "parser.add_argument(\"--dataset\", type=str, default='pathvqa', choices=('pathvqa', 'ovqa', 'slake'))\n",
        "parser.add_argument(\"--dataset_path\", type=str, default=\"vqa_datasets/\")\n",
        "args = parser.parse_args([])\n",
        "\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "if not os.path.exists(args.out_dir):\n",
        "    os.makedirs(args.out_dir)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=args.warmup_steps,\n",
        "    num_training_steps=args.epochs * len(train_loader),\n",
        ")\n",
        "\n",
        "valid_loader = accelerator.prepare(valid_loader)\n",
        "\n",
        "best_valid_loss = float(\"inf\")\n",
        "counter = 0\n",
        "n_epochs = args.epochs\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    with tqdm(total=args.batch_size * len(train_loader)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0.0\n",
        "        total_rocauc = 0.0\n",
        "        for i, (prefix, labels, tokens, mask, q_len) in enumerate(train_loader):\n",
        "            with accelerator.accumulate(model):\n",
        "                prefix = prefix.type(torch.float32).to(device)\n",
        "                tokens = tokens.type(torch.long).to(device)\n",
        "                mask = mask.type(torch.long).to(device)\n",
        "                q_len = q_len.type(torch.long).to(device)\n",
        "                outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                logits = outputs.logits\n",
        "                loss = 0.\n",
        "\n",
        "                shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "\n",
        "                for b in range(logits.size(0)):\n",
        "                    condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                    condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "\n",
        "                    loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                loss=loss/logits.size(0)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                total_loss += loss.item()\n",
        "                avg_loss = total_loss / (i+1)\n",
        "                avg_acc = total_acc / (i + 1)\n",
        "                avg_roc = total_rocauc / (i + 1)\n",
        "                desc = f\"Epoch {epoch} - loss {avg_loss:.20f} -accuracy {avg_acc:.4f} -auc {avg_roc:.4f}\"\n",
        "                epoch_pbar.set_description(desc)\n",
        "                epoch_pbar.update(prefix.shape[0])\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    with tqdm(total=args.batch_size * len(valid_loader)) as epoch_pbar:\n",
        "        epoch_pbar.set_description(f\"VAL Epoch {epoch}\")\n",
        "        for i, (prefix, labels, tokens, mask,q_len) in enumerate(valid_loader):\n",
        "            torch.cuda.empty_cache()\n",
        "            prefix = prefix.type(torch.float32).to(device)\n",
        "            tokens = tokens.type(torch.long).to(device)\n",
        "            mask = mask.type(torch.long).to(device)\n",
        "            q_len = q_len.type(torch.long).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(prefix, labels, tokens, mask, q_len, batch_size=args.batch_size)\n",
        "                logits = outputs.logits\n",
        "                loss = 0.\n",
        "                shift = 10 if args.setting==\"p_tuning\" or args.setting==\"prompttuning\" else 0\n",
        "                for b in range(logits.size(0)):\n",
        "                    condensed_tokens = tokens[b,q_len[b]+model.prefix_length+1:]\n",
        "                    condensed_logits = logits[b,shift+q_len[b]+model.prefix_length:-1]\n",
        "                    loss+= nnf.cross_entropy(condensed_logits.reshape(-1,logits.shape[-1]), condensed_tokens.flatten(), ignore_index=0)\n",
        "                loss=loss/logits.size(0)\n",
        "                total_loss += loss.item()\n",
        "            avg_val_loss = total_loss / (i + 1)\n",
        "            avg_acc = total_acc / (i + 1)\n",
        "            avg_roc = total_rocauc / (i + 1)\n",
        "            desc = f\"VAL Epoch {epoch} - loss {avg_val_loss:.20f} -acc {avg_acc:.4f} -roc {avg_roc:.4f}\"\n",
        "            epoch_pbar.set_description(desc)\n",
        "            epoch_pbar.update(prefix.shape[0])\n",
        "\n",
        "    if avg_val_loss < best_valid_loss:\n",
        "        best_valid_loss = avg_val_loss\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(args.out_dir, f\"open_ended_latest.pt\"))\n",
        "\n",
        "    scheduler.step()\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\n",
        "        \"VAL epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s\".format(\n",
        "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time\n",
        "        )\n",
        "    )\n",
        "    if avg_val_loss > avg_loss:\n",
        "        counter += 1\n",
        "    if counter == 5:\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "id": "fV3_o0ScxDI6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "203ce600-c1b7-47fd-e53d-749941f4bc17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 0 - loss 4.39535212862318847016 -accuracy 0.0000 -auc 0.0000: 100%|██████████| 9936/9936 [08:49<00:00, 18.78it/s]\n",
            "VAL Epoch 0 - loss 3.00385642538265296153 -acc 0.0000 -roc 0.0000: 100%|██████████| 3136/3136 [01:08<00:00, 45.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL epoch 1/30 \t loss=4.3954 \t val_loss=3.0039 \t time=613.67s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - loss 2.98002032844387754196 -accuracy 0.0000 -auc 0.0000:  16%|█▌        | 1568/9936 [01:21<07:14, 19.27it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-323fd224ee52>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2125\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data  import DataLoader\n",
        "\n",
        "class EndoVis18VQAGPTSentence(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head = '../dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa2/Sentence/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.Resize((300,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                q_s, an_s = line.split('|')\n",
        "                q_s = q_s.split('&')\n",
        "                an_s = an_s.split(('&'))\n",
        "                for i in range(len(q_s)):\n",
        "                    q_a = q_s[i]+'|'+an_s[i]\n",
        "                    # print(file, q_a)\n",
        "                    self.vqas.append([file, q_a])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img loc[3],\n",
        "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        answer = '<|sep|> '+answer\n",
        "\n",
        "\n",
        "        return img, question, answer\n",
        "\n",
        "\n",
        "# data location\n",
        "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "val_seq = [1, 5, 16]\n",
        "\n",
        "folder_head = 'datasets_my/EndoVis-18-VQA/seq_'\n",
        "folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "valid_loader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "print('training sample:', len(train_dataset), 'val sample:', len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8GMeoGEy5Hr",
        "outputId": "ca656db6-52c8-4cee-f7c4-6bf8b8f59d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "training sample: 10574 val sample: 3216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2RoeUOhPErX"
      },
      "source": [
        "Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvA3l8GpPG7y"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLSnki7sPIfM"
      },
      "source": [
        "Running Script:<br>\n",
        "%cd /content/Project_Gen\n",
        "import os\n",
        "os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "!python train_SGPT_V2_Sentence.py \\\n",
        "--lr=0.00001 \\\n",
        "--checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_' \\\n",
        "--dataset_type='m18' \\\n",
        "--tokenizer_ver='gpt2v1' \\\n",
        "--model_ver='efvlegpt2rs18' \\\n",
        "--model_subver='v1' \\\n",
        "--vis_pos_emb='zeroes'\\\n",
        "--batch_size=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48GwkJQvPArS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ac10d4-0cb5-446d-fdb9-24b3bd8cbfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project_Gen\n",
            "device = cuda\n",
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "model params:  174607680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 139/265 [03:20<03:18,  1.57s/it]"
          ]
        }
      ],
      "source": [
        "%cd /content/Project_Gen\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch.backends.cudnn as cudnn\n",
        "# from model.EFGPT2Sentence import EFVLEGPT2RS18Sentence\n",
        "# from dataloader.dataloaderGPT2Sentence import EndoVis18VQAGPTSentence\n",
        "# from utils import AverageMeter, save_clf_checkpoint, adjust_learning_rate\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from transformers import  VisualBertConfig, GPT2Config\n",
        "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
        "from transformers import GPT2LMHeadModel\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import ViTFeatureExtractor, AutoFeatureExtractor\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def save_clf_checkpoint(checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, Acc):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'Acc': Acc,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer}\n",
        "    filename = checkpoint_dir + 'Best.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "\n",
        "class EndoVis18VQAGPTSentence(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head = '../dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa2/Sentence/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.Resize((300,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                q_s, an_s = line.split('|')\n",
        "                q_s = q_s.split('&')\n",
        "                an_s = an_s.split(('&'))\n",
        "                for i in range(len(q_s)):\n",
        "                    q_a = q_s[i]+'|'+an_s[i]\n",
        "                    # print(file, q_a)\n",
        "                    self.vqas.append([file, q_a])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img loc[3],\n",
        "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        answer = '<|sep|> '+answer\n",
        "\n",
        "\n",
        "        return img, question, answer\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class EFVLEGPT2RS18Sentence(nn.Module):\n",
        "    def __init__(self, model_subver = 'v3', tokenizer_len=50258, vis_pos_emb = None):\n",
        "        super(EFVLEGPT2RS18Sentence, self).__init__()\n",
        "        '''\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
        "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
        "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
        "        '''\n",
        "\n",
        "        self.sub_ver = model_subver\n",
        "        self.vis_pos_emb = vis_pos_emb\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
        "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "        self.img_feature_extractor.fc = new_fc\n",
        "        self.visual_embedder = nn.Linear(512, 768)\n",
        "\n",
        "        ## word_embedding default GPT2 word embedding\n",
        "        gpt2configuration = GPT2Config()\n",
        "        word_embedder = GPT2Model(gpt2configuration)\n",
        "        word_embedder.resize_token_embeddings(tokenizer_len)\n",
        "        self.word_embedder = word_embedder.wte\n",
        "\n",
        "        ## GPT2 visual context aware decoder\n",
        "        self.VCAdecoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "    def forward(self, question, img, answer):\n",
        "\n",
        "        ## image encoder features\n",
        "        img_feature = self.img_feature_extractor(img)\n",
        "        img_feature = torch.unsqueeze(img_feature, dim=1)\n",
        "\n",
        "        ## visual Embedding : id type 1, pos: zero / incremental\n",
        "        visual_embeds = self.visual_embedder(img_feature)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "        visual_attention_mask = visual_attention_mask.to(device)\n",
        "\n",
        "        ## question embedding:\n",
        "        question['input_ids'] = question['input_ids'].to(device)\n",
        "        question_embeds = self.word_embedder(question['input_ids'])\n",
        "        question_attention_mask = question['attention_mask'].to(device)\n",
        "\n",
        "        ## answer embedding\n",
        "        answer['input_ids'] = answer['input_ids'].to(device)\n",
        "        answer_embeds = self.word_embedder(answer['input_ids'])\n",
        "        answer_attention_mask = answer['attention_mask'].to(device)\n",
        "\n",
        "        ## token type of zero and position id for question\n",
        "        question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        question_position_id = torch.arange(0,question_embeds.size()[1])\n",
        "        question_position_id = torch.unsqueeze(question_position_id,0)\n",
        "        question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
        "        question_position_id = question_position_id.to(device)\n",
        "        question_len = len(question_position_id[0])\n",
        "        visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        visual_len = len(visual_position_id[0])\n",
        "        answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
        "        answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
        "        answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
        "        answer_position_id += (question_len+visual_len)\n",
        "        answer_position_id = answer_position_id.to(device)\n",
        "\n",
        "        ## question first\n",
        "        inputs_embeds = torch.cat((question_embeds, visual_embeds, answer_embeds), dim=1)\n",
        "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask, answer_attention_mask), dim=1)\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            token_type_ids = torch.cat((question_id_type, visual_id_type, answer_id_type), dim=1)\n",
        "            position_ids = torch.cat((question_position_id, visual_position_id, answer_position_id), dim=1)\n",
        "\n",
        "\n",
        "        ## VCA_GPT2 decoder\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
        "        else:\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, ( visual_features, questions, answers) in enumerate(tqdm(train_dataloader),0):\n",
        "\n",
        "        # prepare questions and answers\n",
        "        question_list = []\n",
        "        answer_list = []\n",
        "        for question in questions: question_list.append(question)\n",
        "        for answer in answers: answer_list.append(answer)\n",
        "\n",
        "        question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "        answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "        # Visual features\n",
        "        visual_features = visual_features.to(device)\n",
        "        visual_len = 80\n",
        "\n",
        "        # model forward(question, img, answer)\n",
        "        logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "        # only consider loss on reference summary just like seq2seq models\n",
        "        idx = args.question_len + 1\n",
        "\n",
        "        shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "        shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "        shift_labels = shift_labels.to(device)\n",
        "\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "\n",
        "    print(\"Epoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (visual_features, questions, answers) in enumerate(tqdm(val_loader),0):\n",
        "\n",
        "            # prepare questions and answers\n",
        "            question_list = []\n",
        "            answer_list = []\n",
        "            for question in questions: question_list.append(question)\n",
        "            for answer in answers: answer_list.append(answer)\n",
        "\n",
        "            if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "                answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = visual_features.to(device)\n",
        "                visual_len = 80\n",
        "\n",
        "            # model forward(question, img, answer)\n",
        "            logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "\n",
        "            # only consider loss on reference summary just like seq2seq models\n",
        "            idx = args.question_len + 1\n",
        "            shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "            shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "\n",
        "            # copy for logits and labels for sentence decoding and blue-4 score calculation\n",
        "            logits_copy = logits.clone()\n",
        "            shift_labels_copy = shift_labels.clone()\n",
        "\n",
        "            # loss calculation\n",
        "            shift_labels = shift_labels.to(device)\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # references    - Ground truth answer\n",
        "            answer_GT_dec = tokenizer.batch_decode(shift_labels_copy, skip_special_tokens= True)\n",
        "            for answer_GT_dec_i in answer_GT_dec: references.append([answer_GT_dec_i.split()])\n",
        "            # print(references)\n",
        "\n",
        "            # Hypotheses - predicted answer\n",
        "            _, answer_Gen_id = torch.max(logits_copy, dim=2)\n",
        "            answer_Gen_dec = tokenizer.batch_decode(answer_Gen_id, skip_special_tokens= True)\n",
        "            for answer_Gen_dec_i in answer_Gen_dec: hypotheses.append(answer_Gen_dec_i.split())\n",
        "            # print(hypotheses)\n",
        "\n",
        "\n",
        "        # Calculate BLEU1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "        print(\"Epoch: {}/{} EVA LOSS: {:.6f} BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
        "          (epoch, args.epochs, total_loss.avg, metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=50,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                            help=' 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2rs18/m18/v3_p_qf_',      help='m18/c80')\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                         help='m18/c80')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--model_subver',   default= 'v3',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--answer_len',     default= 35,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2rs18',                               help='efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--vis_pos_emb',    default= 'pos',                                         help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    args = get_arg()\n",
        "    args.lr = 0.00005\n",
        "    args.epochs = 2\n",
        "    args.checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_'\n",
        "    args.dataset_type='m18'\n",
        "    args.tokenizer_ver='gpt2v1'\n",
        "    args.model_ver='efvlegpt2rs18'\n",
        "    args.model_subver='v1'\n",
        "    args.vis_pos_emb='zeroes'\n",
        "    args.batch_size=40\n",
        "    os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer_length = len(tokenizer)\n",
        "\n",
        "    # data location\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = 'datasets/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "    val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "    model = EFVLEGPT2RS18Sentence(model_subver = args.model_subver, tokenizer_len=len(tokenizer), vis_pos_emb = args.vis_pos_emb)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    #evsluation\n",
        "    # checkpoint = torch.load(args.checkpoint, map_location=str(device))\n",
        "    # start_epoch = checkpoint['epoch']\n",
        "    # epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "    # model = checkpoint['model']\n",
        "    # optimizer = checkpoint['optimizer']\n",
        "    # final_args = checkpoint['final_args']\n",
        "    # for key in final_args.keys(): args.__setattr__(key, final_args[key])\n",
        "\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    model = model.to(device)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = CrossEntropyLoss().to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        # validation\n",
        "        metrics = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "            best_results[0] = metrics[\"Bleu_4\"]\n",
        "            best_epoch[0] = epoch\n",
        "            save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0])\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sph3wTEeJueY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}