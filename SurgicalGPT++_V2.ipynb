{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/Project_Gen/blob/main/SurgicalGPT%2B%2B_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvn6yKZCMHhU"
      },
      "source": [
        "Downlaod code and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p3dRC8LKXUN",
        "outputId": "8a3ded77-e9b5-41f5-f5c8-93ce4ad4b7b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Project_Gen'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 49 (delta 21), reused 26 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (49/49), 20.27 KiB | 2.53 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "/content/Project_Gen/datasets\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "To: /content/Project_Gen/datasets/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [00:34<00:00, 78.1MB/s]\n",
            "/content/Project_Gen\n"
          ]
        }
      ],
      "source": [
        "#Download code\n",
        "!git clone https://github.com/mobarakol/Project_Gen.git\n",
        "!mkdir /content/Project_Gen/datasets\n",
        "%cd /content/Project_Gen/datasets\n",
        "\n",
        "#Download Dataset\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip\n",
        "\n",
        "%cd /content/Project_Gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2RoeUOhPErX"
      },
      "source": [
        "Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvA3l8GpPG7y",
        "outputId": "a0a0fb21-1572-4bf2-aa59-b02b1260c60f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLSnki7sPIfM"
      },
      "source": [
        "Running Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdqD0wzeOnoP",
        "outputId": "51bc59f1-c566-445f-f2f4-4f30f16a5f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project_Gen\n",
            "2023-11-04 11:18:11.976547: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-04 11:18:11.976605: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-04 11:18:11.976650: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-04 11:18:13.080322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "efvlegpt2rs18 v1 zeroes m18 1e-05 checkpoints/efvlegpt2rs18/m18_v1_z_qf_\n",
            "device = cuda\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:00<00:00, 15.4MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 49.5MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 95.0MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 4.88MB/s]\n",
            "Total files: 1560 | Total question: 10574\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Total files: 447 | Total question: 3216\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 151MB/s]\n",
            "Downloading model.safetensors: 100% 548M/548M [00:02<00:00, 214MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 795kB/s]\n",
            "model params:  174607680\n",
            "100% 5287/5287 [13:06<00:00,  6.72it/s]\n",
            "Epoch: 1/80 Loss: 0.095402 AVG_Loss: 0.404892\n",
            "100% 1608/1608 [02:44<00:00,  9.75it/s]\n",
            "Epoch: 1/80 EVA LOSS: 0.058222 BLEU-1 0.707408 BLEU2 0.680432 BLEU3 0.654438 BLEU-4 0.616821\n",
            " 10% 523/5287 [01:37<14:50,  5.35it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Project_Gen/train_SGPT_V2_Sentence.py\", line 347, in <module>\n",
            "    train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
            "  File \"/content/Project_Gen/train_SGPT_V2_Sentence.py\", line 70, in train\n",
            "    logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Project_Gen/model/EFGPT2Sentence.py\", line 135, in forward\n",
            "    out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1074, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n",
            "    outputs = block(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 427, in forward\n",
            "    feed_forward_hidden_states = self.mlp(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 356, in forward\n",
            "    hidden_states = self.c_proj(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\", line 107, in forward\n",
            "    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Project_Gen\n",
        "import os\n",
        "os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "!python train_SGPT_V2_Sentence.py \\\n",
        "--lr=0.00001 \\\n",
        "--checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_' \\\n",
        "--dataset_type='m18' \\\n",
        "--tokenizer_ver='gpt2v1' \\\n",
        "--model_ver='efvlegpt2rs18' \\\n",
        "--model_subver='v1' \\\n",
        "--vis_pos_emb='zeroes'\\\n",
        "--batch_size=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48GwkJQvPArS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a6da18f-2208-4ab6-e08b-bd1f2e122d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project_Gen\n",
            "device = cuda\n",
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "model params:  174607680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▋     | 4908/10574 [11:04<11:09,  8.46it/s]"
          ]
        }
      ],
      "source": [
        "%cd /content/Project_Gen\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch.backends.cudnn as cudnn\n",
        "from model.EFGPT2Sentence import EFVLEGPT2RS18Sentence\n",
        "from dataloader.dataloaderGPT2Sentence import EndoVis18VQAGPTSentence\n",
        "from utils import AverageMeter, save_clf_checkpoint, adjust_learning_rate\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class EFVLEGPT2RS18Sentence(nn.Module):\n",
        "    def __init__(self, model_subver = 'v3', tokenizer_len=50258, vis_pos_emb = None):\n",
        "        super(EFVLEGPT2RS18Sentence, self).__init__()\n",
        "        '''\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
        "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
        "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
        "        '''\n",
        "\n",
        "        self.sub_ver = model_subver\n",
        "        self.vis_pos_emb = vis_pos_emb\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
        "            new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "            self.img_feature_extractor.fc = new_fc\n",
        "        elif self.sub_ver == 'v2' or self.sub_ver =='v3':\n",
        "            self.img_feature_extractor = torch.nn.Sequential(*(list(self.img_feature_extractor.children())[:-2]))\n",
        "\n",
        "        ## Visual_embedding\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v2':\n",
        "            # visual bert embedding\n",
        "            VB_config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "            VB_config.visual_embedding_dim = 512\n",
        "            visualbert = VisualBertModel(config=VB_config)\n",
        "            self.visual_embedder = visualbert.embeddings.visual_projection\n",
        "        elif self.sub_ver == 'v1' or self.sub_ver =='v3':\n",
        "            self.visual_embedder = nn.Linear(512, 768)\n",
        "\n",
        "        ## word_embedding default GPT2 word embedding\n",
        "        gpt2configuration = GPT2Config()\n",
        "        word_embedder = GPT2Model(gpt2configuration)\n",
        "        word_embedder.resize_token_embeddings(tokenizer_len)\n",
        "        self.word_embedder = word_embedder.wte\n",
        "\n",
        "        ## GPT2 visual context aware decoder\n",
        "        self.VCAdecoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "    def forward(self, question, img, answer):\n",
        "\n",
        "        ## image encoder features\n",
        "        img_feature = self.img_feature_extractor(img)\n",
        "\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
        "            img_feature = torch.unsqueeze(img_feature, dim=1)\n",
        "        if self.sub_ver == 'v2'or self.sub_ver =='v3':\n",
        "            img_feature = torch.flatten(img_feature, start_dim=2)\n",
        "            img_feature = img_feature.permute((0,2,1))\n",
        "\n",
        "\n",
        "        ## visual Embedding : id type 1, pos: zero / incremental\n",
        "        visual_embeds = self.visual_embedder(img_feature)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "        visual_attention_mask = visual_attention_mask.to(device)\n",
        "\n",
        "        ## question embedding:\n",
        "        question['input_ids'] = question['input_ids'].to(device)\n",
        "        question_embeds = self.word_embedder(question['input_ids'])\n",
        "        question_attention_mask = question['attention_mask'].to(device)\n",
        "\n",
        "        ## answer embedding\n",
        "        answer['input_ids'] = answer['input_ids'].to(device)\n",
        "        answer_embeds = self.word_embedder(answer['input_ids'])\n",
        "        answer_attention_mask = answer['attention_mask'].to(device)\n",
        "\n",
        "        ## token type and position id for question\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            question_position_id = torch.arange(0,question_embeds.size()[1])\n",
        "            question_position_id = torch.unsqueeze(question_position_id,0)\n",
        "            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
        "            question_position_id = question_position_id.to(device)\n",
        "            question_len = len(question_position_id[0])\n",
        "\n",
        "        ## token type and position id for vision\n",
        "        if self.vis_pos_emb == 'zeroes':\n",
        "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        elif self.vis_pos_emb == 'pos':\n",
        "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            visual_position_id = torch.arange(0,visual_embeds.size()[1])\n",
        "            visual_position_id = torch.unsqueeze(visual_position_id,0)\n",
        "            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n",
        "            visual_position_id += (question_len)\n",
        "            visual_position_id = visual_position_id.to(device)\n",
        "        visual_len = len(visual_position_id[0])\n",
        "\n",
        "        ## token type and position id for answer\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
        "            answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
        "            answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
        "            answer_position_id += (question_len+visual_len)\n",
        "            answer_position_id = answer_position_id.to(device)\n",
        "\n",
        "\n",
        "        ## question first\n",
        "        inputs_embeds = torch.cat((question_embeds, visual_embeds, answer_embeds), dim=1)\n",
        "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask, answer_attention_mask), dim=1)\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            token_type_ids = torch.cat((question_id_type, visual_id_type, answer_id_type), dim=1)\n",
        "            position_ids = torch.cat((question_position_id, visual_position_id, answer_position_id), dim=1)\n",
        "\n",
        "\n",
        "        ## VCA_GPT2 decoder\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
        "        else:\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, (_, visual_features, questions, answers) in enumerate(tqdm(train_dataloader),0):\n",
        "\n",
        "        # prepare questions and answers\n",
        "        question_list = []\n",
        "        answer_list = []\n",
        "        for question in questions: question_list.append(question)\n",
        "        for answer in answers: answer_list.append(answer)\n",
        "\n",
        "        if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "            answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "        # Visual features\n",
        "        if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "        else:\n",
        "            visual_features = visual_features.to(device)\n",
        "            visual_len = 80\n",
        "\n",
        "        # model forward(question, img, answer)\n",
        "        logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "        # only consider loss on reference summary just like seq2seq models\n",
        "        idx = args.question_len + 1\n",
        "\n",
        "        shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "        shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "        shift_labels = shift_labels.to(device)\n",
        "\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "\n",
        "    print(\"Epoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (_, visual_features, questions, answers) in enumerate(tqdm(val_loader),0):\n",
        "\n",
        "            # prepare questions and answers\n",
        "            question_list = []\n",
        "            answer_list = []\n",
        "            for question in questions: question_list.append(question)\n",
        "            for answer in answers: answer_list.append(answer)\n",
        "\n",
        "            if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "                answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = visual_features.to(device)\n",
        "                visual_len = 80\n",
        "\n",
        "            # model forward(question, img, answer)\n",
        "            logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "\n",
        "            # only consider loss on reference summary just like seq2seq models\n",
        "            idx = args.question_len + 1\n",
        "            shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "            shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "\n",
        "            # copy for logits and labels for sentence decoding and blue-4 score calculation\n",
        "            logits_copy = logits.clone()\n",
        "            shift_labels_copy = shift_labels.clone()\n",
        "\n",
        "            # loss calculation\n",
        "            shift_labels = shift_labels.to(device)\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # references    - Ground truth answer\n",
        "            answer_GT_dec = tokenizer.batch_decode(shift_labels_copy, skip_special_tokens= True)\n",
        "            for answer_GT_dec_i in answer_GT_dec: references.append([answer_GT_dec_i.split()])\n",
        "            # print(references)\n",
        "\n",
        "            # Hypotheses - predicted answer\n",
        "            _, answer_Gen_id = torch.max(logits_copy, dim=2)\n",
        "            answer_Gen_dec = tokenizer.batch_decode(answer_Gen_id, skip_special_tokens= True)\n",
        "            for answer_Gen_dec_i in answer_Gen_dec: hypotheses.append(answer_Gen_dec_i.split())\n",
        "            # print(hypotheses)\n",
        "\n",
        "\n",
        "        # Calculate BLEU1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "        print(\"Epoch: {}/{} EVA LOSS: {:.6f} BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
        "          (epoch, args.epochs, total_loss.avg, metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=50,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                            help=' 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2rs18/m18/v3_p_qf_',      help='m18/c80')\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                         help='m18/c80')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--model_subver',   default= 'v3',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--answer_len',     default= 35,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2rs18',                               help='efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--vis_pos_emb',    default= 'pos',                                         help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    args = get_arg()\n",
        "    args.checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_'\n",
        "    args.dataset_type='m18'\n",
        "    args.tokenizer_ver='gpt2v1'\n",
        "    args.model_ver='efvlegpt2rs18'\n",
        "    args.model_subver='v1'\n",
        "    args.vis_pos_emb='zeroes'\n",
        "    args.batch_size=1\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "\n",
        "    if args.dataset_type == 'm18':\n",
        "        '''\n",
        "        Train and test dataloader for EndoVis18\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'gpt2v1':\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer_length = len(tokenizer)\n",
        "\n",
        "        # data location\n",
        "        train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "        val_seq = [1, 5, 16]\n",
        "\n",
        "        folder_head = 'datasets/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "            train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "            val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "\n",
        "    # Initialize / load checkpoint\n",
        "    if args.checkpoint is None:\n",
        "        if args.model_ver == 'efvlegpt2rs18':\n",
        "            model = EFVLEGPT2RS18Sentence(model_subver = args.model_subver, tokenizer_len=len(tokenizer), vis_pos_emb = args.vis_pos_emb)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(args.checkpoint, map_location=str(device))\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        # best_Acc = checkpoint['Acc']\n",
        "        model = checkpoint['model']\n",
        "        optimizer = checkpoint['optimizer']\n",
        "        final_args = checkpoint['final_args']\n",
        "        for key in final_args.keys(): args.__setattr__(key, final_args[key])\n",
        "\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    model = model.to(device)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = CrossEntropyLoss().to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        # validation\n",
        "        metrics = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "            best_results[0] = metrics[\"Bleu_4\"]\n",
        "            best_epoch[0] = epoch\n",
        "            save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0])\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sph3wTEeJueY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}