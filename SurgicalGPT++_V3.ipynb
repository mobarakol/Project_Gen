{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/Project_Gen/blob/main/SurgicalGPT%2B%2B_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvn6yKZCMHhU"
      },
      "source": [
        "Downlaod code and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p3dRC8LKXUN",
        "outputId": "83c1f86e-3fed-4bae-e3b9-46e5568998aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Project_Gen'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 61 (delta 29), reused 25 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (61/61), 28.77 KiB | 507.00 KiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/Project_Gen/datasets\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "To: /content/Project_Gen/datasets/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [00:23<00:00, 113MB/s]\n",
            "/content/Project_Gen\n"
          ]
        }
      ],
      "source": [
        "#Download code\n",
        "!git clone https://github.com/mobarakol/Project_Gen.git\n",
        "!mkdir /content/Project_Gen/datasets\n",
        "%cd /content/Project_Gen/datasets\n",
        "\n",
        "#Download Dataset\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip\n",
        "\n",
        "%cd /content/Project_Gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2RoeUOhPErX"
      },
      "source": [
        "Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvA3l8GpPG7y",
        "outputId": "8bb0bc67-cf50-427e-a9b1-18e5945c82de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLSnki7sPIfM"
      },
      "source": [
        "Running Script:<br>\n",
        "%cd /content/Project_Gen\n",
        "import os\n",
        "os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "!python train_SGPT_V2_Sentence.py \\\n",
        "--lr=0.00001 \\\n",
        "--checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_' \\\n",
        "--dataset_type='m18' \\\n",
        "--tokenizer_ver='gpt2v1' \\\n",
        "--model_ver='efvlegpt2rs18' \\\n",
        "--model_subver='v1' \\\n",
        "--vis_pos_emb='zeroes'\\\n",
        "--batch_size=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48GwkJQvPArS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550c40d8-502a-4dca-870a-d23f9bfbefe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Project_Gen\n",
            "device = cuda\n",
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "model params:  174607680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:19<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/80 Loss: 0.152326 AVG_Loss: 1.622419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:56<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/80 EVA LOSS: 0.160428 BLEU-1 0.670892 BLEU2 0.629873 BLEU3 0.589946 BLEU-4 0.537232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:17<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/80 Loss: 0.062247 AVG_Loss: 0.118006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:55<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/80 EVA LOSS: 0.074745 BLEU-1 0.718891 BLEU2 0.693080 BLEU3 0.669087 BLEU-4 0.634676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:18<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3/80 Loss: 0.099176 AVG_Loss: 0.066021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:52<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3/80 EVA LOSS: 0.064464 BLEU-1 0.718844 BLEU2 0.692304 BLEU3 0.666923 BLEU-4 0.630615\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:14<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4/80 Loss: 0.037802 AVG_Loss: 0.049515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:52<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4/80 EVA LOSS: 0.069155 BLEU-1 0.714949 BLEU2 0.687266 BLEU3 0.661070 BLEU-4 0.623998\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:10<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5/80 Loss: 0.031362 AVG_Loss: 0.040842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:54<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5/80 EVA LOSS: 0.070899 BLEU-1 0.719260 BLEU2 0.690648 BLEU3 0.664234 BLEU-4 0.626910\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:11<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6/80 Loss: 0.026872 AVG_Loss: 0.035301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:53<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6/80 EVA LOSS: 0.073516 BLEU-1 0.719824 BLEU2 0.693628 BLEU3 0.668040 BLEU-4 0.631517\n",
            "\n",
            "Epochs since last improvement: 4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:08<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7/80 Loss: 0.017535 AVG_Loss: 0.030413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:53<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7/80 EVA LOSS: 0.062585 BLEU-1 0.726191 BLEU2 0.702678 BLEU3 0.679797 BLEU-4 0.646505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:15<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8/80 Loss: 0.049948 AVG_Loss: 0.026694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:55<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8/80 EVA LOSS: 0.068095 BLEU-1 0.726734 BLEU2 0.700425 BLEU3 0.675776 BLEU-4 0.640445\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:12<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9/80 Loss: 0.034958 AVG_Loss: 0.023344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:54<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9/80 EVA LOSS: 0.068432 BLEU-1 0.725209 BLEU2 0.699516 BLEU3 0.674573 BLEU-4 0.638735\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:18<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/80 Loss: 0.037256 AVG_Loss: 0.022382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:56<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/80 EVA LOSS: 0.074819 BLEU-1 0.727834 BLEU2 0.703136 BLEU3 0.679034 BLEU-4 0.644070\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:18<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11/80 Loss: 0.015074 AVG_Loss: 0.018527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:55<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11/80 EVA LOSS: 0.074352 BLEU-1 0.739552 BLEU2 0.715165 BLEU3 0.691154 BLEU-4 0.656483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:20<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12/80 Loss: 0.016104 AVG_Loss: 0.017211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:56<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12/80 EVA LOSS: 0.090719 BLEU-1 0.736037 BLEU2 0.711996 BLEU3 0.687959 BLEU-4 0.653137\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:22<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13/80 Loss: 0.001971 AVG_Loss: 0.015116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:55<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13/80 EVA LOSS: 0.085064 BLEU-1 0.778228 BLEU2 0.754043 BLEU3 0.730938 BLEU-4 0.697721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:22<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14/80 Loss: 0.041140 AVG_Loss: 0.013325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:55<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14/80 EVA LOSS: 0.102000 BLEU-1 0.739801 BLEU2 0.715131 BLEU3 0.691031 BLEU-4 0.656574\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:24<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15/80 Loss: 0.007768 AVG_Loss: 0.011808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:58<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15/80 EVA LOSS: 0.114725 BLEU-1 0.731055 BLEU2 0.704251 BLEU3 0.678765 BLEU-4 0.642518\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:24<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16/80 Loss: 0.027480 AVG_Loss: 0.009563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:56<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16/80 EVA LOSS: 0.113201 BLEU-1 0.729145 BLEU2 0.703400 BLEU3 0.679394 BLEU-4 0.644934\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:29<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17/80 Loss: 0.006710 AVG_Loss: 0.009564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:59<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17/80 EVA LOSS: 0.120084 BLEU-1 0.741424 BLEU2 0.716399 BLEU3 0.691671 BLEU-4 0.656302\n",
            "\n",
            "Epochs since last improvement: 4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:31<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18/80 Loss: 0.008984 AVG_Loss: 0.008353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:57<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18/80 EVA LOSS: 0.142670 BLEU-1 0.745167 BLEU2 0.720420 BLEU3 0.696186 BLEU-4 0.660938\n",
            "\n",
            "Epochs since last improvement: 5\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000040\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:20<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19/80 Loss: 0.009171 AVG_Loss: 0.006334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:56<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19/80 EVA LOSS: 0.147773 BLEU-1 0.742395 BLEU2 0.717605 BLEU3 0.693414 BLEU-4 0.658750\n",
            "\n",
            "Epochs since last improvement: 6\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:26<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20/80 Loss: 0.000904 AVG_Loss: 0.005426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:57<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20/80 EVA LOSS: 0.143184 BLEU-1 0.745517 BLEU2 0.721344 BLEU3 0.697217 BLEU-4 0.662310\n",
            "\n",
            "Epochs since last improvement: 7\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:37<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21/80 Loss: 0.001459 AVG_Loss: 0.005168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:57<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21/80 EVA LOSS: 0.131424 BLEU-1 0.752416 BLEU2 0.727781 BLEU3 0.703933 BLEU-4 0.669362\n",
            "\n",
            "Epochs since last improvement: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:25<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22/80 Loss: 0.006202 AVG_Loss: 0.004811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:54<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22/80 EVA LOSS: 0.150441 BLEU-1 0.764478 BLEU2 0.740211 BLEU3 0.716587 BLEU-4 0.682416\n",
            "\n",
            "Epochs since last improvement: 9\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:23<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23/80 Loss: 0.010873 AVG_Loss: 0.004295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:54<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23/80 EVA LOSS: 0.134500 BLEU-1 0.790838 BLEU2 0.767280 BLEU3 0.745087 BLEU-4 0.712943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:22<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24/80 Loss: 0.014913 AVG_Loss: 0.004452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:56<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24/80 EVA LOSS: 0.138199 BLEU-1 0.790877 BLEU2 0.767255 BLEU3 0.744424 BLEU-4 0.711661\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 150/265 [03:42<02:08,  1.12s/it]"
          ]
        }
      ],
      "source": [
        "%cd /content/Project_Gen\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch.backends.cudnn as cudnn\n",
        "# from model.EFGPT2Sentence import EFVLEGPT2RS18Sentence\n",
        "from dataloader.dataloaderGPT2Sentence import EndoVis18VQAGPTSentence\n",
        "from utils import AverageMeter, save_clf_checkpoint, adjust_learning_rate\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from transformers import  VisualBertConfig, GPT2Config\n",
        "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
        "from transformers import GPT2LMHeadModel\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class EFVLEGPT2RS18Sentence(nn.Module):\n",
        "    def __init__(self, model_subver = 'v3', tokenizer_len=50258, vis_pos_emb = None):\n",
        "        super(EFVLEGPT2RS18Sentence, self).__init__()\n",
        "        '''\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
        "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
        "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
        "        '''\n",
        "\n",
        "        self.sub_ver = model_subver\n",
        "        self.vis_pos_emb = vis_pos_emb\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
        "            new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "            self.img_feature_extractor.fc = new_fc\n",
        "        elif self.sub_ver == 'v2' or self.sub_ver =='v3':\n",
        "            self.img_feature_extractor = torch.nn.Sequential(*(list(self.img_feature_extractor.children())[:-2]))\n",
        "\n",
        "        ## Visual_embedding\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v2':\n",
        "            # visual bert embedding\n",
        "            VB_config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "            VB_config.visual_embedding_dim = 512\n",
        "            visualbert = VisualBertModel(config=VB_config)\n",
        "            self.visual_embedder = visualbert.embeddings.visual_projection\n",
        "        elif self.sub_ver == 'v1' or self.sub_ver =='v3':\n",
        "            self.visual_embedder = nn.Linear(512, 768)\n",
        "\n",
        "        ## word_embedding default GPT2 word embedding\n",
        "        gpt2configuration = GPT2Config()\n",
        "        word_embedder = GPT2Model(gpt2configuration)\n",
        "        word_embedder.resize_token_embeddings(tokenizer_len)\n",
        "        self.word_embedder = word_embedder.wte\n",
        "\n",
        "        ## GPT2 visual context aware decoder\n",
        "        self.VCAdecoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "    def forward(self, question, img, answer):\n",
        "\n",
        "        ## image encoder features\n",
        "        img_feature = self.img_feature_extractor(img)\n",
        "\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
        "            img_feature = torch.unsqueeze(img_feature, dim=1)\n",
        "        if self.sub_ver == 'v2'or self.sub_ver =='v3':\n",
        "            img_feature = torch.flatten(img_feature, start_dim=2)\n",
        "            img_feature = img_feature.permute((0,2,1))\n",
        "\n",
        "\n",
        "        ## visual Embedding : id type 1, pos: zero / incremental\n",
        "        visual_embeds = self.visual_embedder(img_feature)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "        visual_attention_mask = visual_attention_mask.to(device)\n",
        "\n",
        "        ## question embedding:\n",
        "        question['input_ids'] = question['input_ids'].to(device)\n",
        "        question_embeds = self.word_embedder(question['input_ids'])\n",
        "        question_attention_mask = question['attention_mask'].to(device)\n",
        "\n",
        "        ## answer embedding\n",
        "        answer['input_ids'] = answer['input_ids'].to(device)\n",
        "        answer_embeds = self.word_embedder(answer['input_ids'])\n",
        "        answer_attention_mask = answer['attention_mask'].to(device)\n",
        "\n",
        "        ## token type and position id for question\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            question_position_id = torch.arange(0,question_embeds.size()[1])\n",
        "            question_position_id = torch.unsqueeze(question_position_id,0)\n",
        "            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
        "            question_position_id = question_position_id.to(device)\n",
        "            question_len = len(question_position_id[0])\n",
        "\n",
        "        ## token type and position id for vision\n",
        "        if self.vis_pos_emb == 'zeroes':\n",
        "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        elif self.vis_pos_emb == 'pos':\n",
        "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            visual_position_id = torch.arange(0,visual_embeds.size()[1])\n",
        "            visual_position_id = torch.unsqueeze(visual_position_id,0)\n",
        "            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n",
        "            visual_position_id += (question_len)\n",
        "            visual_position_id = visual_position_id.to(device)\n",
        "        visual_len = len(visual_position_id[0])\n",
        "\n",
        "        ## token type and position id for answer\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
        "            answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
        "            answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
        "            answer_position_id += (question_len+visual_len)\n",
        "            answer_position_id = answer_position_id.to(device)\n",
        "\n",
        "\n",
        "        ## question first\n",
        "        inputs_embeds = torch.cat((question_embeds, visual_embeds, answer_embeds), dim=1)\n",
        "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask, answer_attention_mask), dim=1)\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            token_type_ids = torch.cat((question_id_type, visual_id_type, answer_id_type), dim=1)\n",
        "            position_ids = torch.cat((question_position_id, visual_position_id, answer_position_id), dim=1)\n",
        "\n",
        "\n",
        "        ## VCA_GPT2 decoder\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
        "        else:\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, (_, visual_features, questions, answers) in enumerate(tqdm(train_dataloader),0):\n",
        "\n",
        "        # prepare questions and answers\n",
        "        question_list = []\n",
        "        answer_list = []\n",
        "        for question in questions: question_list.append(question)\n",
        "        for answer in answers: answer_list.append(answer)\n",
        "\n",
        "        if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "            answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "\n",
        "        # Visual features\n",
        "        if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "        else:\n",
        "            visual_features = visual_features.to(device)\n",
        "            visual_len = 80\n",
        "\n",
        "        # model forward(question, img, answer)\n",
        "        logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "        # only consider loss on reference summary just like seq2seq models\n",
        "        idx = args.question_len + 1\n",
        "\n",
        "        shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "        shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "        shift_labels = shift_labels.to(device)\n",
        "\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "\n",
        "    print(\"Epoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (_, visual_features, questions, answers) in enumerate(tqdm(val_loader),0):\n",
        "\n",
        "            # prepare questions and answers\n",
        "            question_list = []\n",
        "            answer_list = []\n",
        "            for question in questions: question_list.append(question)\n",
        "            for answer in answers: answer_list.append(answer)\n",
        "\n",
        "            if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "                answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = visual_features.to(device)\n",
        "                visual_len = 80\n",
        "\n",
        "            # model forward(question, img, answer)\n",
        "            logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "\n",
        "            # only consider loss on reference summary just like seq2seq models\n",
        "            idx = args.question_len + 1\n",
        "            shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "            shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "\n",
        "            # copy for logits and labels for sentence decoding and blue-4 score calculation\n",
        "            logits_copy = logits.clone()\n",
        "            shift_labels_copy = shift_labels.clone()\n",
        "\n",
        "            # loss calculation\n",
        "            shift_labels = shift_labels.to(device)\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # references    - Ground truth answer\n",
        "            answer_GT_dec = tokenizer.batch_decode(shift_labels_copy, skip_special_tokens= True)\n",
        "            for answer_GT_dec_i in answer_GT_dec: references.append([answer_GT_dec_i.split()])\n",
        "            # print(references)\n",
        "\n",
        "            # Hypotheses - predicted answer\n",
        "            _, answer_Gen_id = torch.max(logits_copy, dim=2)\n",
        "            answer_Gen_dec = tokenizer.batch_decode(answer_Gen_id, skip_special_tokens= True)\n",
        "            for answer_Gen_dec_i in answer_Gen_dec: hypotheses.append(answer_Gen_dec_i.split())\n",
        "            # print(hypotheses)\n",
        "\n",
        "\n",
        "        # Calculate BLEU1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "        print(\"Epoch: {}/{} EVA LOSS: {:.6f} BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
        "          (epoch, args.epochs, total_loss.avg, metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=50,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                            help=' 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2rs18/m18/v3_p_qf_',      help='m18/c80')\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                         help='m18/c80')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--model_subver',   default= 'v3',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--answer_len',     default= 20,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2rs18',                               help='efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--vis_pos_emb',    default= 'pos',                                         help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    args = get_arg()\n",
        "    args.lr = 0.00005\n",
        "    args.epochs = 80\n",
        "    args.checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_'\n",
        "    args.dataset_type='m18'\n",
        "    args.tokenizer_ver='gpt2v1'\n",
        "    args.model_ver='efvlegpt2rs18'\n",
        "    args.model_subver='v1'\n",
        "    args.vis_pos_emb='zeroes'\n",
        "    args.batch_size=40\n",
        "    os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "\n",
        "    if args.dataset_type == 'm18':\n",
        "        '''\n",
        "        Train and test dataloader for EndoVis18\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'gpt2v1':\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer_length = len(tokenizer)\n",
        "\n",
        "        # data location\n",
        "        train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "        val_seq = [1, 5, 16]\n",
        "\n",
        "        folder_head = 'datasets/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "            train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "            val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "\n",
        "    # Initialize / load checkpoint\n",
        "    if args.checkpoint is None:\n",
        "        if args.model_ver == 'efvlegpt2rs18':\n",
        "            model = EFVLEGPT2RS18Sentence(model_subver = args.model_subver, tokenizer_len=len(tokenizer), vis_pos_emb = args.vis_pos_emb)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(args.checkpoint, map_location=str(device))\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        # best_Acc = checkpoint['Acc']\n",
        "        model = checkpoint['model']\n",
        "        optimizer = checkpoint['optimizer']\n",
        "        final_args = checkpoint['final_args']\n",
        "        for key in final_args.keys(): args.__setattr__(key, final_args[key])\n",
        "\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    model = model.to(device)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = CrossEntropyLoss().to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        # validation\n",
        "        metrics = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "            best_results[0] = metrics[\"Bleu_4\"]\n",
        "            best_epoch[0] = epoch\n",
        "            save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0])\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sph3wTEeJueY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}